<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Awesome Chatbot</title>
  
  <subtitle>Blog by Handsome Geeks</subtitle>
  <link href="/awesome-chatbot/atom.xml" rel="self"/>
  
  <link href="https://bupt.github.io/awesome-chatbot/"/>
  <updated>2018-10-19T10:20:56.402Z</updated>
  <id>https://bupt.github.io/awesome-chatbot/</id>
  
  <author>
    <name>BUPTer</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TPU for developers，and the FREE Colab</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/10/19/google-tpu-colab/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/10/19/google-tpu-colab/</id>
    <published>2018-10-18T16:00:00.000Z</published>
    <updated>2018-10-19T10:20:56.402Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><h3 id="TPU-For-Developers-SLIDE"><a href="#TPU-For-Developers-SLIDE" class="headerlink" title="TPU For Developers (SLIDE)"></a>TPU For Developers (SLIDE)</h3><p>Link: <a href="https://docs.google.com/presentation/d/1iodAZkOX0YMnUwohgQqNsbEkhR0zAnO-jncK9SkJ69o/edit?usp=sharing" target="_blank" rel="noopener">https://docs.google.com/presentation/d/1iodAZkOX0YMnUwohgQqNsbEkhR0zAnO-jncK9SkJ69o/edit?usp=sharing</a></p><h3 id="TensorFlow-Keras-TPUs"><a href="#TensorFlow-Keras-TPUs" class="headerlink" title="TensorFlow Keras TPUs"></a>TensorFlow Keras TPUs</h3><p>Link: <a href="https://docs.google.com/presentation/d/12gKSBBg-nmpxvioLwwnZQCalv4frfxYq25ClvT1ZdqE/edit?usp=sharing" target="_blank" rel="noopener">https://docs.google.com/presentation/d/12gKSBBg-nmpxvioLwwnZQCalv4frfxYq25ClvT1ZdqE/edit?usp=sharing</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h3 id=&quot;TPU-For-Developers-SLIDE&quot;&gt;&lt;a href=&quot;#TPU-For-Developers-SLIDE&quot; clas
      
    
    </summary>
    
      <category term="Slides" scheme="https://bupt.github.io/awesome-chatbot/categories/Slides/"/>
    
    
      <category term="tpu" scheme="https://bupt.github.io/awesome-chatbot/tags/tpu/"/>
    
      <category term="tensorflow" scheme="https://bupt.github.io/awesome-chatbot/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>胶囊（Capsule）网络</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/10/15/dynamic-routing-between-capsules/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/10/15/dynamic-routing-between-capsules/</id>
    <published>2018-10-14T16:00:00.000Z</published>
    <updated>2018-10-17T13:48:41.682Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><h2 id="手写笔记"><a href="#手写笔记" class="headerlink" title="手写笔记"></a>手写笔记</h2><p>第一部分根据国外博客进行make sense的直观理解。<br>第二部分根据苏神博客的进行数学方面的推导加强理解。</p><div class="row"><iframe src="https://drive.google.com/file/d/1mA1dSM1q-12pfpr75842DWO0zcvsIFbt/preview" style="width:100%; height:550px"></iframe></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://kexue.fm/archives/5155" target="_blank" rel="noopener">https://kexue.fm/archives/5155</a></li><li><a href="https://kexue.fm/archives/5112" target="_blank" rel="noopener">https://kexue.fm/archives/5112</a></li><li><a href="https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418" target="_blank" rel="noopener">https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418</a></li><li><a href="https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66" target="_blank" rel="noopener">https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h2 id=&quot;手写笔记&quot;&gt;&lt;a href=&quot;#手写笔记&quot; class=&quot;headerlink&quot; title=&quot;手写笔记&quot;&gt;&lt;/a&gt;手写笔记&lt;/h2
      
    
    </summary>
    
      <category term="Classical_paper" scheme="https://bupt.github.io/awesome-chatbot/categories/Classical-paper/"/>
    
    
      <category term="capsule" scheme="https://bupt.github.io/awesome-chatbot/tags/capsule/"/>
    
      <category term="dynamic_routing" scheme="https://bupt.github.io/awesome-chatbot/tags/dynamic-routing/"/>
    
  </entry>
  
  <entry>
    <title>变分自编码器及其应用</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/10/03/auto-encoding-variational-bayes/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/10/03/auto-encoding-variational-bayes/</id>
    <published>2018-10-02T16:00:00.000Z</published>
    <updated>2018-10-19T10:17:50.425Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>笔记主要参考变分自编码器的原论文<a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank" rel="noopener">《Auto-Encoding Variational Bayes》</a>，与<a href="https://kexue.fm/archives/5253" target="_blank" rel="noopener">苏神的博客</a></p><h2 id="VAE模型"><a href="#VAE模型" class="headerlink" title="VAE模型"></a>VAE模型</h2><p>VAE的目标（与GAN相同）：希望构建一个从隐变量$Z$生成目标数据$X$的模型。<br>VAE的核心是：<strong>进行分布之间的变换。</strong></p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvzqckwnkhj20my0aegn1.jpg" alt=""></p><p>VAE的Encoder有两个，一个用来计算均值，一个用来计算方差。</p><h3 id="问题所在"><a href="#问题所在" class="headerlink" title="问题所在"></a>问题所在</h3><p>但生成模型的难题是判断生成分布与真实分布的相似度。（即我们只知道抽样结果，不知道分布表达式）<br><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvuzgu3recj20or0c7400.jpg" alt=""></p><h3 id="大部分教程所述的VAE"><a href="#大部分教程所述的VAE" class="headerlink" title="大部分教程所述的VAE"></a>大部分教程所述的VAE</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvxa4kzerpj20qt0dy40q.jpg" alt=""><br>模型思路是：先从标准正态分布中采样一个Z，然后根据Z来算一个X。<br>若VAE结构确实是这个图的话，我们其实完全不清楚：究竟经过重新采样出来的$Z_k$，是不是还对应着原来的$X_k$。</p><p>其实，在整个VAE模型中，我们并没有去使用$p(Z)$（隐变量空间的分布）是正态分布的假设，我们用的是假设$p(Z|X)$（后验分布）是正态分布!</p><p>但是，训练好的神经网络<br>并且VAE会让所有的$P(Z|X)$都向标准正态分布看齐：<br>$$p(Z)=\sum_X p(Z|X)p(X)=\sum_X \mathcal{N}(0,I)p(X)=\mathcal{N}(0,I) \sum_X p(X) = \mathcal{N}(0,I)$$</p><h3 id="真实的VAE"><a href="#真实的VAE" class="headerlink" title="真实的VAE"></a>真实的VAE</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvuzt27ie8j20rf0imjv6.jpg" alt=""><br>VAE是为每个样本构造<strong>专属</strong>的正态分布，然后采样来重构。</p><p>但是神经网络经过训练之后的方差会接近0。采样只会得到确定的结果。</p><p>因此还需要使所有的正态分布都向<strong>标准正态分布</strong>（模型的假设）看齐。为了使所有的P(Z|X)都向$\mathcal{N}(0,I)$看齐，我们需要：</p><h4 id="编码器：使用神经网络方法拟合参数"><a href="#编码器：使用神经网络方法拟合参数" class="headerlink" title="编码器：使用神经网络方法拟合参数"></a>编码器：使用神经网络方法拟合参数</h4><p>构建两个神经网络：$\mu_k=f_1(X_k), log_{\sigma^2}=f_2(X_k)$来拟合均值和方差。当二者尽量接近零的时候，分布也就达到了$\mathcal{N}(0,I)$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z_mean = Dense(latent_dim)(h)</span><br><span class="line">z_log_var = Dense(latent_dim)(h)</span><br></pre></td></tr></table></figure></p><p>针对两个损失的比例选取，使用KL散度$KL(N(\mu,\sigma^2)||N(0,I))$作为额外的loss。上式的计算结果为：</p><p>$$\mathcal{L}<em>{\mu,\sigma^2}=\frac{1}{2} \sum</em>{i=1}^d \Big(\mu_{(i)}^2 + \sigma_{(i)}^2 - \log \sigma_{(i)}^2 - 1\Big)$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kl_loss = - <span class="number">0.5</span> * K.sum(<span class="number">1</span> + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h4 id="解码器：保证生成能力"><a href="#解码器：保证生成能力" class="headerlink" title="解码器：保证生成能力"></a>解码器：保证生成能力</h4><p>我们最终的目标则是最小化误差$\mathcal{D}(\hat{X_k},X_k)^2$。</p><p>解码器重构$X$的过程是希望没噪声的，而$KL loss$则希望有高斯噪声的，两者是对立的。所以，VAE跟GAN一样，内部其实是包含了一个对抗的过程，只不过它们两者是混合起来，共同进化的。</p><h4 id="reparameterization-trick（重参数技巧）"><a href="#reparameterization-trick（重参数技巧）" class="headerlink" title="reparameterization trick（重参数技巧）"></a>reparameterization trick（重参数技巧）</h4><p>在反向传播优化均值和方差的过程中，“采样”操作是<strong>不可导</strong>的，但是采样的结果是可导的。</p><p>因此可以利用标准正太分布采样出的$\epsilon$直接估算$Z=\mu+\epsilon\times\sigma$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampling</span><span class="params">(args)</span>:</span></span><br><span class="line">    <span class="string">"""Reparameterization trick by sampling fr an isotropic unit Gaussian.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Arguments:</span></span><br><span class="line"><span class="string">        args (tensor): mean and log of variance of Q(z|X)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Returns:</span></span><br><span class="line"><span class="string">        z (tensor): sampled latent vector</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    z_mean, z_log_var = args</span><br><span class="line">    batch = K.shape(z_mean)[<span class="number">0</span>]</span><br><span class="line">    dim = K.int_shape(z_mean)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># by default, random_normal has mean=0 and std=1.0</span></span><br><span class="line">    epsilon = K.random_normal(shape=(batch, dim))</span><br><span class="line">    <span class="keyword">return</span> z_mean + K.exp(<span class="number">0.5</span> * z_log_var) * epsilon</span><br></pre></td></tr></table></figure></p><p>于是“采样”操作不再参与梯度下降，改为采样的结果参与，使得整个模型可以训练。</p><h2 id="DEMO-基于CNN和VAE的作诗机器人"><a href="#DEMO-基于CNN和VAE的作诗机器人" class="headerlink" title="DEMO:基于CNN和VAE的作诗机器人"></a>DEMO:基于CNN和VAE的作诗机器人</h2><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18gy1fvuzt1xl9cj20rf0imjv6.jpg" alt=""><br>先将每个字embedding为向量，然后用层叠CNN来做编码，接着池化得到一个encoder的结果，根据这个结果生成计算均值和方差，然后生成正态分布并重新采样。在解码截断，由于现在只有一个encoder的输出结果，而最后要输出多个字，所以先接了多个不同的全连接层，得到多样的输出，然后再接着全连接层。</p><h4 id="GCNN-Gated-Convolutional-Networks"><a href="#GCNN-Gated-Convolutional-Networks" class="headerlink" title="GCNN(Gated Convolutional Networks)"></a>GCNN(Gated Convolutional Networks)</h4><p>这里的CNN不是普通的CNN+ReLU，而是facebook提出的GCNN，其实就是做两个不同的、外形一样的CNN，一个不加激活函数，一个用sigmoid激活，然后把结果乘起来。这样一来sigmoid那部分就相当于起到了一个“门（gate）”的作用。</p><h2 id="参考与引用"><a href="#参考与引用" class="headerlink" title="参考与引用"></a>参考与引用</h2><ul><li><a href="https://kexue.fm/archives/5253" target="_blank" rel="noopener">https://kexue.fm/archives/5253</a></li><li><a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6114.pdf</a></li><li><a href="https://kexue.fm/archives/5332" target="_blank" rel="noopener">https://kexue.fm/archives/5332</a></li><li><a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" target="_blank" rel="noopener">https://jaan.io/what-is-variational-autoencoder-vae-tutorial/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;笔记主要
      
    
    </summary>
    
      <category term="VAE" scheme="https://bupt.github.io/awesome-chatbot/categories/VAE/"/>
    
    
      <category term="note" scheme="https://bupt.github.io/awesome-chatbot/tags/note/"/>
    
      <category term="code" scheme="https://bupt.github.io/awesome-chatbot/tags/code/"/>
    
  </entry>
  
  <entry>
    <title>对象差分注意力机制</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/09/21/object-difference-attention/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/09/21/object-difference-attention/</id>
    <published>2018-09-20T16:00:00.000Z</published>
    <updated>2018-10-17T13:48:28.684Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><!-- 论文基本信息：方便查阅和追踪 --><!-- 论文基本信息的获取：1. 直接从论文pdf中获取2. 从paperweekly首页上方搜索论文；若未检索到，点击推荐论文输入论文名即可自动获取信息--><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：Object-Difference Attention: A Simple Relational Attention for Visual Question Answering</p></li><li><p>论文链接：<a href="http://www.acmmm.org/2018/accepted-papers/" target="_blank" rel="noopener">http://www.acmmm.org/2018/accepted-papers/</a></p></li><li><p>论文源码：</p><ul><li>None</li></ul></li><li><p>关于作者：</p><ul><li>吴晨飞，北邮AI Lab博士</li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。  </li></ul></li></ol><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><!-- Ex: 论文摘要的中文翻译最近对话生成的神经模型为会话代理生成响应提供了很大的希望，但往往是短视的，一次预测一个话语而忽略它们对未来结果的影响。对未来的对话方向进行建模对于产生连贯，有趣的对话至关重要，这种对话需要传统的NLP对话模式借鉴强化学习。在本文中，我们将展示如何整合这些目标，应用深度强化学习来模拟聊天机器人对话中的未来奖励。该模型模拟两个虚拟代理之间的对话，使用策略梯度方法来奖励显示三个有用会话属性的序列：信息性，连贯性和易于回答（与前瞻性功能相关）。我们在多样性，长度以及人类评判方面评估我们的模型，表明所提出的算法产生了更多的交互式响应，并设法在对话模拟中促进更持久的对话。这项工作标志着基于对话的长期成功学习神经对话模型的第一步。--><p>注意机制极大地促进了视觉问答技术(VQA)的发展。注意力分配在注意力机制中起着至关重要的作用，它根据对象(如图像区域或定界框)回答问题的重要性对图像中的对象(如图像区域或包围盒)进行不同的权重。现有的工作大多集中在融合图像特征和文本特征来计算注意力分布，而不需要比较<strong>不同的图像对象</strong>。作为注意力的一个主要属性，<strong>分离度</strong>取决于不同对象之间的比较。这种比较为更好地分配注意力提供了更多的信息。为了实现对目标的可感知性，我们提出了一种对象差分注意(ODA)方法，通过在图像中实现不同图像对象之间的差值运算来计算注意概率。实验结果表明，我们基于ODA的VQA模型得到了最先进的结果。此外，还提出了一种关系注意的一般形式。除了ODA之外，本文还介绍了其他一些相关的注意事项。实验结果表明，这些关系关注在不同类型的问题上都有优势。</p><h2 id="对象差分注意力机制：视觉问答中一个简单的关系注意力机制"><a href="#对象差分注意力机制：视觉问答中一个简单的关系注意力机制" class="headerlink" title="对象差分注意力机制：视觉问答中一个简单的关系注意力机制"></a>对象差分注意力机制：视觉问答中一个简单的关系注意力机制</h2><!-- Ex: ## 强化学习在对话生成领域的应用 --><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><h4 id="本文术语"><a href="#本文术语" class="headerlink" title="本文术语"></a>本文术语</h4><!-- 针对论文中不常用的术语进行简短的解释，方便读者理解 --><ol><li><p>序列编码的方式：</p><ol><li><strong>RNN</strong>: $y_t=f(y_{t-1},x_t)$</li><li><strong>CNN</strong>: $y_t=f(x_{t-1},x_t,x_{t+1})$</li><li><strong>Attention</strong>: $y_t=f(x_t, A, B), if A = B = X: Self Attention$</li></ol></li><li><p>注意力机制的例子<br>$$Attention(Q,K,V)$$</p></li><li><p>应用于VQA的注意力机制编年史：</p><ol><li>one-step linear fusion</li><li>multi-step linear fusion</li><li>bilinear fusion</li><li>multi-feature attention</li></ol></li><li><p>Mutan机制</p></li></ol><h4 id="论文写作动机"><a href="#论文写作动机" class="headerlink" title="论文写作动机"></a>论文写作动机</h4><!-- 当前研究领域存在的问题Ex:标准的Seq-to-Seq模型用于对话系统时常常使用MLE作为模型的评价标准，但这往往导致下面两个主要缺点：系统倾向于产生一些普适性的回应，也就是dull response，这些响应可以回答很多问题但却并不是我们想要的，我们想要的是有趣、多样性、丰富的回应；系统的回复不具有前瞻性，有时会导致陷入死循环，导致对话轮次较少。也就是产生的响应没有考虑对方是否容易回答的情况。--><ol><li>现有的工作大多集中在融合图像特征和文本特征来计算注意力分布，而<strong>忽略了</strong>比较不同的图像对象之间的差异。<br> <img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvisv9uyyhj20i10cw46l.jpg" alt=""><br> 如上图，想要回答出问题<code>图中最高的花是什么？</code>，我们建立的模型就需要不仅仅关注潜在答案<code>玫瑰</code>，也应该关注<code>兰花</code>。</li><li>如何合理分配现有问题的注意力？</li></ol><h3 id="解决问题的方法"><a href="#解决问题的方法" class="headerlink" title="解决问题的方法"></a>解决问题的方法</h3><h4 id="玫瑰例子"><a href="#玫瑰例子" class="headerlink" title="玫瑰例子"></a>玫瑰例子</h4><p>对于回答<code>图中最高的花是什么？</code>，一共分几步？</p><ol><li>找到图中所有的花。</li><li>比较不同的花对于正确答案的重要性。</li></ol><p>正确的答案就会在<strong>比较</strong>的过程中产生。若以这个例子作为启发，一种新型的注意力机制的思路便产生了：ODA在问题的指导下，通过将每个图像对象与其他所有对象进行对比，计算出图像中物体的注意注意力分布。</p><h4 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjiq8dptpj20pw0bdgqo.jpg" alt=""></p><ol><li><p>将数据Embedding</p><ol><li>$V^f=RCNN(image)$,其中$v^f$是一个$m\times{d_v}$维的embedding，代表拉出的$m$个框。</li><li>$Q^f=GRU(question)$，其中$Q^f$代表$d_q$维的问题embedding。</li><li>$V=relu(Conv1d(V^f))$</li><li>$Q=relu(Linear(Q^f))$</li></ol></li><li><p>对象差分注意力<br>$$\hat{V}=softmax([(V_i-V_j)\odot{Q}]_{m\odot{md}}W_f)^{T}V$$<br>该模型的优点：</p><ol><li>通过对比(差分))，我们可以选择更重要的对象。</li><li>计算复杂度相对与传统注意力机制模型（Mutan）低。</li><li>”即插即用“的特性使得该模型十分容易应用到其他领域。</li></ol></li><li><p>决策阶段</p><ol><li><p>通过对$\hat{V}$计算$p$次，并且将结果拼接在一起。<br>$$\hat{Z}=[\hat{V}^{1};\hat{V}^{2};…;\hat{V}^{p}]$$</p><blockquote><p>可以参考Attention is all you need模型的multi-head</p></blockquote></li><li>将图片的特征和问题的特征相结合<br>$$H=\sum^s_{s=1}(\hat{Z}W_v^{(s)}\odot{QW_q^{(s)}})$$</li><li>预测<br>$$\hat{a}=\sigma(W_{h}H)$$</li></ol></li></ol><h4 id="扩展：相关性注意力"><a href="#扩展：相关性注意力" class="headerlink" title="扩展：相关性注意力"></a>扩展：相关性注意力</h4><p>针对模型中$(V_i-V_j)\odot{Q}$部分进行扩展，可以得到不同类型的注意力机制<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjt8ggw48j20dk06emya.jpg" alt=""></p><h3 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>VQA1.0 dataset</li><li>VQA2.0 dataset</li><li>COCO-QA dataset</li></ul><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ul><li>针对VQA1.0和VQA2.0，使用准确率：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtavlwoxj209701hgli.jpg" alt=""></li><li>针对COCO_QA使用：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtbn6b1pj207m00tdfo.jpg" alt=""></li></ul><h4 id="实验结果评价"><a href="#实验结果评价" class="headerlink" title="实验结果评价"></a>实验结果评价</h4><ul><li>在VQA1.0上与最先进的模型对比<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtf0nyn4j20qs0c8wh1.jpg" alt=""></li><li>在VQA2.0上与最先进的模型对比<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtfgxjdxj20ht05qmy9.jpg" alt=""></li><li>在VQA3.0上与最先进的模型对比<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvjtg34t3dj20mm05twfl.jpg" alt=""></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从感性的角度来说，对象差分注意力机制符合人类根据图片回答问题的思考过程。未来的研究方向应该是通过对世界的常识性知识建立一个世界模型，通过先验知识减少计算量和对大量带有标签的数据的依赖性。</p><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><!--Ex:1. https://www.paperweekly.site/papers/notes/2212. https://scholar.google.com/--><ol><li><a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">https://kexue.fm/archives/4765</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;!-- 论文基本信息：方便查阅和追踪 --&gt;
&lt;!-- 论文基本信息的获取：
1. 直接从论文pdf中获取
2. 从paperweekly首页上方
      
    
    </summary>
    
      <category term="VQA" scheme="https://bupt.github.io/awesome-chatbot/categories/VQA/"/>
    
    
      <category term="attention" scheme="https://bupt.github.io/awesome-chatbot/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>世界模型的解读</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/09/21/world-model-to-learn-them-all/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/09/21/world-model-to-learn-them-all/</id>
    <published>2018-09-20T16:00:00.000Z</published>
    <updated>2018-09-26T12:37:39.898Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><!-- 论文基本信息：方便查阅和追踪 --><!-- 论文基本信息的获取：从paperweekly首页上方搜索论文；若未检索到，点击推荐论文输入论文名即可自动获取信息--><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：World Models</p><!-- Ex: 1. 论文名：Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs. --></li><li><p>论文链接：<a href="https://arxiv.org/pdf/1803.10122.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.10122.pdf</a></p><!-- Ex: 2. https://arxiv.org/abs/1606.01541  --></li><li><p>论文源码：</p><ul><li><a href="https://worldmodels.github.io/" target="_blank" rel="noopener">https://worldmodels.github.io/</a></li></ul></li></ol><!--    - https://github.com/liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow    - https://github.com/agsarthak/Goal-oriented-Dialogue-Systems--><ol><li><p>关于作者：</p><!-- 建议从google schoolar获取详细信息  - first_author: position, times_cited--><ul><li>David Ha：</li><li>Jurgen Schimidhuber: LSTM之父，无需多言</li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。</li></ul></li></ol><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><p>通过探索并且建立流行的强化学习环境的生成神经网络模型。<strong>世界模型</strong>可以在无监督的情况下快速训练，以学习环境的压缩时空表示。通过使用从世界模型中提取的特征作为Agent的输入，可以训练一个非常紧凑和简单的策略来解决所需的任务。甚至可以训练Agent完全在它自己的世界模型所产生的<strong>梦</strong>中，并将这个策略转移回实际环境中。</p><h2 id="世界模型"><a href="#世界模型" class="headerlink" title="世界模型"></a>世界模型</h2><h3 id="论文写作动机"><a href="#论文写作动机" class="headerlink" title="论文写作动机"></a>论文写作动机</h3><ol><li><p>哲学问题：人类究竟如何认识世界？<br> 人类通过有限的感知能力（眼睛、鼻子、耳朵、皮肤），逐渐建立一个自己的<strong>心智模型</strong>。人类的一切决策和动作则均根据每个人自己的内部模型的<strong>预测</strong>而产生。</p></li><li><p>人类如何处理日常生活的信息流？<br> 通过<strong>注意力机制</strong>学习客观世界时空方面的抽象表达。</p></li><li><p>人类的潜意识如何工作？<br> 以棒球为例子，击球手在如此短的时间（短于视觉信号到达大脑的时间！）内需要作出何时击球的动作。<br> 人类可以完成击球的原因便是因为人类天生的心智模型可以预测棒球的运动路线。</p></li><li><p>我们是否可以让模型根据环境自觉建立特有的模型进行自学习？</p></li><li><p>Jurgen历史性的工作总结：强化学习背景下的RNN-based世界模型!</p></li></ol><h3 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h3><h4 id="总览Agent模型"><a href="#总览Agent模型" class="headerlink" title="总览Agent模型"></a>总览Agent模型</h4><ol><li><p>视觉感知元件：<strong>压缩</strong>视觉获取到的信息</p></li><li><p>记忆元件：根据历史信息对客观环境进行<strong>预测</strong></p></li><li><p>决策模块：根据视觉感知元件和记忆元件选择<strong>行动</strong></p></li></ol><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvm2l4tc84j20wf0ledkn.jpg" alt=""></p><h4 id="VAE-V-Model"><a href="#VAE-V-Model" class="headerlink" title="VAE(V) Model"></a>VAE(V) Model</h4><p>Agent通过VAE可以从观察的每一帧中学习出抽象的、压缩的表示。</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvm3sg25puj20wn09zq7g.jpg" alt=""></p><h4 id="MDN-RNN-M-Model"><a href="#MDN-RNN-M-Model" class="headerlink" title="MDN-RNN(M) Model"></a>MDN-RNN(M) Model</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmurcu30uj20u10l8jta.jpg" alt=""><br>其中下一时刻的预测$z_{t+1}$使用概率的形式表示为$P(z_{t+1}|a_t,z_t,h_t)$,其中$a_t$是在$t$时刻的动作。<br>并且在采样阶段，通过调整温度参数$\tau$来控制模型的模糊度。(这个参数对后续训练控制器环节十分有效)</p><p>模型最上方的<strong>MDN</strong>表示<strong>Mixture Density Network</strong>。</p><h4 id="Controller-C-Model"><a href="#Controller-C-Model" class="headerlink" title="Controller(C) Model"></a>Controller(C) Model</h4><p>这个模块用来根据最大累计Reward决定Agent下一个时刻的行动。论文中故意将这个模块设置的尽量小并且简单。</p><p>因此控制器是一个简单的单层线性模型：<br>$$a_t=W_c[z_t h_t]+b_c$$</p><p>特别指出，优化控制器参数的方法不是传统的梯度下降，而是<strong>Covariance-Matrix Adaptation Evolution Strategy</strong></p><h4 id="结合三个模块"><a href="#结合三个模块" class="headerlink" title="结合三个模块"></a>结合三个模块</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmvf1dkd5j20p50ic77b.jpg" alt=""></p><p>通过伪代码表示模型:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rollout</span><span class="params">(controller)</span>:</span></span><br><span class="line">  obs = env.reset()</span><br><span class="line">  h = rnn.initial_state()</span><br><span class="line">  done = <span class="keyword">False</span></span><br><span class="line">  cumulative_reward = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">    z = vae.encode(obs)</span><br><span class="line">    a = controller.action([z, h])</span><br><span class="line">    obs, reward, done = env.step(a)</span><br><span class="line">    cumulative_reward += reward</span><br><span class="line">    h = rnn.forward([a, z, h])</span><br><span class="line">  <span class="keyword">return</span> cumulative_reward</span><br></pre></td></tr></table></figure></p><h3 id="实验设计1"><a href="#实验设计1" class="headerlink" title="实验设计1"></a>实验设计1</h3><p>两个实验的环境均选自<code>OpenAI Gym</code></p><h4 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h4><p>CarRacing-v0(Car Racing Experiment)</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmw8osn7xj20vh0kt43n.jpg" alt=""></p><p>动作空间有</p><ol><li>左转</li><li>右转</li><li>加速</li><li>刹车</li></ol><h4 id="实验实现流程"><a href="#实验实现流程" class="headerlink" title="实验实现流程"></a>实验实现流程</h4><ol><li>根据随机的策略收集10，000次游戏过程</li><li>根据每个游戏过程的每一帧训练VAE模型，输出结果为$z\in \mathcal{R}^{32}$</li><li>训练MDN-RNN模型，输出结果为$P(z_{t+1}|a_t,z_t,h_t)$</li><li>定义控制器（c）,$a_t=W_c[z_t h_t]+b_c$</li><li>使用CMS-ES算法得到最大化累计Reward的$$W_b$与$b_c$</li></ol><p>模型参数共有：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmwg6zv1ej20gg063jru.jpg" alt=""></p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><ol><li><p>只添加V Model(VAE)<br>如果没有M Model(MDN-RNN)模块，控制器的公式便为：$a_t=W_{c}z_t+b_c$。<br>实验结果表明这会导致Agent不稳定的驾驶行为。<br>在这种情况下，尝试控制器添加一层隐含层，虽然实验效果有所提升，但是仍然没能达到很好的效果。</p></li><li><p>世界模型完全体（VAE+MDN-RNN）<br>实验结果表明，Agent驾驶得更加稳定。<br>因为$h_t$包含了当前环境关于未来信息的概率分布，因此Agent可以向一级方程式选手和棒球手一样迅速做出判断。</p></li><li><p>世界模型与其他模型的对比：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmxb7t9zdj20op09odi1.jpg" alt=""></p></li><li><p>对世界模型当前状态$z_{t+1}$进行可视化<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmxeae7jyj20g00g4jwu.jpg" alt=""><br>上图将$\tau$设置为0.25（这个参数可以调节生成环境的模糊程度）</p></li></ol><h3 id="实验设计2"><a href="#实验设计2" class="headerlink" title="实验设计2"></a>实验设计2</h3><p><strong>我们是否可以让Agent在自己的梦境中学习，并且改变其对真实环境的策略</strong><br>如果世界模型对其<strong>目的</strong>有了充分的认识，那么我们就可以使用世界代替Agent真实观察到的环境。（类比我们下楼梯的时候，根本不需要小心翼翼地看着楼梯）<br>最终，Agent将不会直接观察到现实世界，而只会看到世界模型<strong>让</strong>它看到的事物。</p><h4 id="实验环境-1"><a href="#实验环境-1" class="headerlink" title="实验环境"></a>实验环境</h4><p>VizDoom Experiment</p><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvmxq3o7ihj20g10bywio.jpg" alt=""></p><p>游戏目的是控制Agent躲避怪物发出的火球。</p><h4 id="实验实现流程-1"><a href="#实验实现流程-1" class="headerlink" title="实验实现流程"></a>实验实现流程</h4><p>模型的M Model(MDN-RNN)主要负责预测Agent下一时刻（帧）是否会死亡。<br>当Agent在其世界模型中进行训练的时候，便不需要V Model对真实环境的像素进行编码了。</p><ol><li>从随机策略中选取10，000局游戏（同实验一）</li><li>根据每次游戏的每一帧训练VAE模型，得到$z\in \mathcal{R}^{64}$($z$的维度变成了64)，之后使用VAE模型将收集的图像转换为隐空间表示。</li><li>训练MDN-RNN模型，输出结果为$P(z_{t+1},d_{t+1}|a_t,z_t,h_t)$</li><li>定义控制器为$a_t=W_c[z_t h_t]$</li><li>使用CMA-ES算法从世界模型构建的虚拟环境中得到最大化累计生存时间的$W_c$</li></ol><p>模型的参数共有：<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvn0lmh3rdj20li07dwf8.jpg" alt=""></p><h4 id="模糊化世界模型"><a href="#模糊化世界模型" class="headerlink" title="模糊化世界模型"></a>模糊化世界模型</h4><p>通过增加模糊度参数$\tau$，会使得游戏变得更难（世界模型生成的环境更加模糊）。<br>如果Agent在高模糊度参数表现的很好的话，那么在正常模式下通常表现的更好。</p><p>也就是说，即使<strong>V model(VAE)不能够正确的捕捉每一帧全部的信息</strong>，Agent也能够完成真实环境给定的任务。</p><p>实验结果表明，模糊度参数太低相当于没有利用这个参数，但是太高的话模型又相当于”近视“了。因此需要找到一个合适的模糊度参数值。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="泛化：迭代式训练程序"><a href="#泛化：迭代式训练程序" class="headerlink" title="泛化：迭代式训练程序"></a>泛化：迭代式训练程序</h4><ol><li>随机初始化M Model(MDN-RNN)和C Model(Controller)的参数</li><li>对真实环境进行N次试验。保存每次试验的动作$a_t$和观察$x_t$</li><li>训练M Model(MDN-RNN)，得到$P(x_{t+1},r_{t+1},a_{t+1},d_{t+1}|x_t,a_t,h_t)$；训练C Model(Controller)并且M中的最优化期望rewards。</li><li>回到第2步如果任务没有结束</li></ol><p>这个泛化程序的特点是从M model中不仅仅要得到预测的观察$x$和是否结束任务$done$，</p><p>一般的seq2seq模型，倾向于生成安全、普适的响应，因为这种响应更符合语法规则，在训练集中出现频率也较高，最终生成的概率也最大，而有意义的响应生成概率往往比他们小。通过MMI来计算输入输出之间的依赖性和相关性，可以减少模型对他们的生成概率。</p><h4 id="从信息到记忆：海马体的魔术"><a href="#从信息到记忆：海马体的魔术" class="headerlink" title="从信息到记忆：海马体的魔术"></a>从信息到记忆：海马体的魔术</h4><p>神经科学的研究（2017 Foster）发现了海马体重映现象：当动物休息或者睡觉的时候，其大脑会重新放映最近的经历。并且海马体重映现象对巩固记忆十分重要。</p><h4 id="注意力：只关心任务相关的特征"><a href="#注意力：只关心任务相关的特征" class="headerlink" title="注意力：只关心任务相关的特征"></a>注意力：只关心任务相关的特征</h4><p>神经科学的研究（2013 Pi）发现，主要视觉神经元只有在受到奖励的时候才会被从抑制状态激活。这表明人类通常从任务相关的特征中学习，而非接收到的所有特征。（该结论至少在成年人中成立）</p><h4 id="未来的展望"><a href="#未来的展望" class="headerlink" title="未来的展望"></a>未来的展望</h4><p>当前的问题主要出现在M Model(MDN-RNN)上：受限于RNN模型的信息存储能力。人类的大脑能够存储几十年甚至几百年的记忆，但是神经网络会因为梯度消失导致训练困难。</p><p>如果想让Agent可以探索更加复杂的世界，那么未来的工作可能是设计出一个可以<strong>代替MDN-RNN结构的模型</strong>，或者开发出一个<strong>外部记忆模块</strong>。</p><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><ol><li><a href="https://arxiv.org/pdf/1803.10122.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.10122.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;!-- 论文基本信息：方便查阅和追踪 --&gt;
&lt;!-- 论文基本信息的获取：从paperweekly首页上方搜索论文；若未检索到，点击推荐论文输入
      
    
    </summary>
    
      <category term="AI" scheme="https://bupt.github.io/awesome-chatbot/categories/AI/"/>
    
    
      <category term="note" scheme="https://bupt.github.io/awesome-chatbot/tags/note/"/>
    
      <category term="JurgenSchmidhuber" scheme="https://bupt.github.io/awesome-chatbot/tags/JurgenSchmidhuber/"/>
    
  </entry>
  
  <entry>
    <title>demo驱动学习：Image_Caption</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/08/26/show-attend-and-tell-neural-image-caption-generation-with-visual-attention/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/08/26/show-attend-and-tell-neural-image-caption-generation-with-visual-attention/</id>
    <published>2018-08-25T16:00:00.000Z</published>
    <updated>2018-10-17T13:48:28.685Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><h2 id="Introduction-to-demo"><a href="#Introduction-to-demo" class="headerlink" title="Introduction to demo"></a>Introduction to demo</h2><p>Source Code:<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb" target="_blank" rel="noopener">image_captioning_with_attention</a></p><h3 id="Related-Papers"><a href="#Related-Papers" class="headerlink" title="Related Papers"></a>Related Papers</h3><p><a href="https://arxiv.org/pdf/1502.03044.pdf" target="_blank" rel="noopener">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.</a></p><h3 id="Goal-of-this-end2end-model"><a href="#Goal-of-this-end2end-model" class="headerlink" title="Goal of this end2end model"></a>Goal of this end2end model</h3><ol><li>Generate a caption, such as “a surfer riding on a wave”, according to an image.<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fun2xxvjt8j20hs0buamq.jpg" alt=""></li><li>Use an attention based model that enables us to see which parts of the image the model focuses on as it generates a caption.<br><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fun2yatwwhj20zz0ehk1c.jpg" alt=""></li></ol><h3 id="Dateset"><a href="#Dateset" class="headerlink" title="Dateset"></a>Dateset</h3><p><strong>MS-COCO</strong>:This dataset contains &gt;82,000 images, each of which has been annotated with at least 5 different captions.</p><h2 id="Frame-work-of-demo"><a href="#Frame-work-of-demo" class="headerlink" title="Frame work of demo:"></a>Frame work of demo:</h2><ol><li>Download and prepare the MS-COCO dataset</li><li>Limit the size of the training set for faster training</li><li><p>Preprocess the images using InceptionV3: extract features from the last convolutional layer.</p><ol><li>Initialize InceptionV3 and load the pretrained Imagenet weights</li><li>Caching the features extracted from InceptionV3</li></ol></li><li><p>Preprocess and tokenize the captions</p><ol><li>First, tokenize the captions will give us a vocabulary of all the unique words in the data (e.g., “surfing”, “football”, etc).</li><li>Next, limit the vocabulary size to the top 5,000 words to save memory. We’ll replace all other words with the token “UNK” (for unknown).</li><li>Finally, we create a word –&gt; index mapping and vice-versa.</li><li>We will then pad all sequences to the be same length as the longest one.</li></ol></li><li><p>create a tf.data dataset to use for training our model.</p></li></ol><ol start="6"><li><p>Model</p><ol><li>extract the features from the lower convolutional layer of InceptionV3 giving us a vector of shape (8, 8, 2048).</li><li>This vector is then passed through the CNN Encoder(which consists of a single Fully connected layer).</li><li>The RNN(here GRU) attends over the image to predict the next word.</li></ol></li><li><p>Training</p><ol><li>We extract the features stored in the respective .npy files and then pass those features through the encoder.</li><li>The encoder output, hidden state(initialized to 0) and the decoder input (which is the start token) is passed to the decoder.</li><li>The decoder returns the predictions and the decoder hidden state.</li><li>The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.</li><li>Use teacher forcing to decide the next input to the decoder.</li><li>Teacher forcing is the technique where the target word is passed as the next input to the decoder.</li><li>The final step is to calculate the gradients and apply it to the optimizer and backpropagate.</li></ol></li><li><p>Caption</p><ol><li>The evaluate function is similar to the training loop, except we don’t use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.</li><li>Stop predicting when the model predicts the end token.</li><li>And store the attention weights for every time step.</li></ol></li></ol><h2 id="Problems-undesirable"><a href="#Problems-undesirable" class="headerlink" title="Problems undesirable"></a>Problems undesirable</h2><h3 id="Version"><a href="#Version" class="headerlink" title="Version"></a>Version</h3><ul><li>The code requires TensorFlow version <strong>&gt;=1.9</strong>. 1.10.0 is better.</li><li><code>cudatoolkit</code></li></ul><h3 id="GPU-lose-connect"><a href="#GPU-lose-connect" class="headerlink" title="GPU lose connect"></a>GPU lose connect</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h2 id=&quot;Introduction-to-demo&quot;&gt;&lt;a href=&quot;#Introduction-to-demo&quot; class=&quot;heade
      
    
    </summary>
    
      <category term="VQA" scheme="https://bupt.github.io/awesome-chatbot/categories/VQA/"/>
    
    
      <category term="imageCaption" scheme="https://bupt.github.io/awesome-chatbot/tags/imageCaption/"/>
    
  </entry>
  
  <entry>
    <title>Visual-Question-Learning(VQA)学习笔记</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/08/23/visual-question-answering-datasets-algorithms-and-future%20Challenges/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/08/23/visual-question-answering-datasets-algorithms-and-future Challenges/</id>
    <published>2018-08-22T16:00:00.000Z</published>
    <updated>2018-10-17T13:48:28.685Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><!-- 论文基本信息：方便查阅和追踪 --><!-- 论文基本信息的获取：1. 直接从论文pdf中获取2. 从paperweekly首页上方搜索论文；若未检索到，点击推荐论文输入论文名即可自动获取信息--><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><ol><li><p>论文名：Visual Question Answering: Datasets, Algorithms, and Future Challenges</p></li><li><p>论文链接：<a href="https://arxiv.org/pdf/1610.01465.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1610.01465.pdf</a></p><!-- Ex: https://arxiv.org/abs/1606.01541  --></li><li><p>论文源码</p><ul><li>None</li></ul></li><li><p>关于作者</p><ul><li>Kushal Kafle</li><li>Christopher Kanan</li></ul></li><li><p>关于笔记作者：</p><ul><li>朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。  </li></ul></li></ol><h2 id="论文推荐理由"><a href="#论文推荐理由" class="headerlink" title="论文推荐理由"></a>论文推荐理由</h2><!-- Ex: 论文摘要的中文翻译最近对话生成的神经模型为会话代理生成响应提供了很大的希望，但往往是短视的，一次预测一个话语而忽略它们对未来结果的影响。对未来的对话方向进行建模对于产生连贯，有趣的对话至关重要，这种对话需要传统的NLP对话模式借鉴强化学习。在本文中，我们将展示如何整合这些目标，应用深度强化学习来模拟聊天机器人对话中的未来奖励。该模型模拟两个虚拟代理之间的对话，使用策略梯度方法来奖励显示三个有用会话属性的序列：信息性，连贯性和易于回答（与前瞻性功能相关）。我们在多样性，长度以及人类评判方面评估我们的模型，表明所提出的算法产生了更多的交互式响应，并设法在对话模拟中促进更持久的对话。这项工作标志着基于对话的长期成功学习神经对话模型的第一步。 --><p> 视觉问答(Visual Question answering, VQA)是近年来计算机视觉和自然语言处理领域的一个热点问题。在VQA中，一个算法需要回答关于图像的基于文本的问题。自2014年发布第一个VQA数据集以来，已经发布了更多的数据集，并提出了许多算法。在这篇综述中，我们从问题的形成、现有数据集、评估指标和算法的角度，批判性地研究了VQA的当前状态。特别地，我们讨论了当前数据集在正确训练和评估VQA算法方面的局限性。然后，我们详尽地回顾了VQA的现有算法。最后，我们讨论了未来VQA和图像理解研究的可能方向。</p><h2 id="视觉问答：数据集，算法和未来的挑战"><a href="#视觉问答：数据集，算法和未来的挑战" class="headerlink" title="视觉问答：数据集，算法和未来的挑战"></a>视觉问答：数据集，算法和未来的挑战</h2><!-- Ex: ## 强化学习在对话生成领域的应用 --><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><h4 id="VQA的研究价值"><a href="#VQA的研究价值" class="headerlink" title="VQA的研究价值"></a>VQA的研究价值</h4><ol><li><p>大部分计算机视觉任务不能完整的理解图像<br>图像分类、物体检测、动作识别等任务很难获取到物体的<strong>空间位置信息</strong>并且根据它们的属性和关系进行<strong>推理</strong>。</p></li><li><p>人类对<strong>Grand Unified Theory</strong>的痴迷追求</p><ul><li>目标识别任务：图像里面有什么？[分类]</li><li>目标检测任务：图像里面有猫吗？[拉框]</li><li>属性分类任务：图像里面的猫是什么颜色的？</li><li>场景分类：图像是在室内吗？</li><li>计数任务：图像里面有多少猫？<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvah1uwsvij208c04ogn4.jpg" alt=""></li></ul></li><li><p>通过视觉图灵测试：</p><ul><li>基准问题测试</li><li>建立评价指标 </li></ul></li></ol><h3 id="VQA的数据集"><a href="#VQA的数据集" class="headerlink" title="VQA的数据集"></a>VQA的数据集</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvajh4e1wbj20tz09eacb.jpg" alt=""></p><h3 id="VQA的评价标准"><a href="#VQA的评价标准" class="headerlink" title="VQA的评价标准"></a>VQA的评价标准</h3><ul><li>Open-ended(OE): 开放式的</li><li>Multiple Choice(MC): 选择式的</li></ul><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvb6r1njlkj20kp0dmjuj.jpg" alt=""></p><h4 id="流行的评价标准"><a href="#流行的评价标准" class="headerlink" title="流行的评价标准"></a>流行的评价标准</h4><p>选择式任务的评价标准直接使用正确率即可。但是开放式任务的评价标准呢？</p><ol><li>Simple accuracy:<ol><li>Q: What animals are in the photo<br>若<code>dogs</code>是正确答案，那么<code>dog</code>和<code>zebra</code>的惩罚竟然是一样的</li><li>Q: What is in the tree<br>若<code>bald eagle</code>是正确答案，<code>eagle</code>或是<code>bird</code>  与  <code>yes</code>的惩罚竟然也是一样的</li></ol></li><li><p>Wu-Palmer Similarity</p><ol><li>语义相似度<br><code>Black</code>、<code>White</code>两个单词的<code>WUPS score</code>是0.91。所以这可能会给错误答案一个相当高的分数。</li><li>只可以评价单词，句子不可使用</li></ol></li><li><p>$Accuracy_{VQA}=min(\frac{n}{3}, 1)$<br>同样是语义相似度，大致正确就ok: 人为构造一个答案集合，$n$是算法和人类拥有的相同的答案数量。</p></li></ol><h3 id="VQA的算法"><a href="#VQA的算法" class="headerlink" title="VQA的算法"></a>VQA的算法</h3><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbaa7e7d4j20qr098n45.jpg" alt=""><br>存在的算法大致结构均包括：</p><ol><li>提取图像特征</li><li>提取问题特征</li><li>利用特征产生结果的算法</li></ol><h4 id="Baseline和模型性能"><a href="#Baseline和模型性能" class="headerlink" title="Baseline和模型性能"></a>Baseline和模型性能</h4><ol><li>瞎猜最有可能的答案。“yes”/“no”</li><li>MLP(multi-layer percepton)</li></ol><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbapjz9zkj20ky0lradq.jpg" alt=""></p><h4 id="模型架构一览"><a href="#模型架构一览" class="headerlink" title="模型架构一览"></a>模型架构一览</h4><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbasmehclj20qg0l278r.jpg" alt=""></p><ol><li>基于贝叶斯和问题导向的模型</li><li>基于注意力机制的模型<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbb0ttq7cj20r60dj7bs.jpg" alt=""></li><li>非线性池化方法</li></ol><ul><li>MULTI-WORLD: A multi-world approach to question answering about real- world scenes based on uncertain input, NIPS2014</li><li>ASK-NEURon: Ask your neurons: A neural-based ap- proach to answering questions about images, ICCV2015</li><li>ENSEMSBLE: Exploring models and data for image question answering, NIPS2015</li><li>LSTM Q+I: VQA: Visual question answering, ICCV2015</li><li>iBOWIMG: Simple baseline for visual question answering, arxiv</li><li>DPPNET: Image question answering using convolutional neural network with dynamic parameter prediction, CVPR2016</li><li>SMem: Ask, attend and answer: Exploring question-guided spatial attention for visual question answering, ECCV2016</li><li>SAN: Stacked attention networks for image question answering, CVPR2016</li><li>NMN: Deep compositional question answering with neural module networks, CVPR2016</li><li>FDA: A focused dynamic attention model for visual question answering, arxiv2016</li><li>HYBRID: Answer-type prediction for visual question answering, CVPR2016</li><li>DMN+: Dynamic memory networks for visual and textual question answering, ICML2016</li><li>MRN: Multimodal residual learning for visual qa, NIPS2016</li><li>HieCoAtten: Hierarchical question-image co-attention for visual question answering, NIPS2016</li><li>RAU_ResNet: Training recurrent answering units with joint loss minimization for VQA, arxiv2016</li><li>DAN: Dual attention networks for multimodal reasoning and matching, arxiv2016</li><li>MCB+Att: Multi-modal compact bilinear pooling for visual question answering and visual grounding, EMNLP2016</li><li>MLB: Hadamard product for low-rank bilinear pooling, arxiv2016</li><li>AMA: Ask me anything: Free-form visual question answering based on knowledge from external sources, CVPR2016</li><li>MCB-ensemble: Multi-modal compact bilinear pooling for visual question answering and visual grounding, EMNLP2016</li></ul><h3 id="VQA仍然存在很多问题"><a href="#VQA仍然存在很多问题" class="headerlink" title="VQA仍然存在很多问题"></a>VQA仍然存在很多问题</h3><p>虽然VQA已经取得了长足的进步，但是现有的算法仍然距离人类有巨大的差距。<br><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fvbbxjxinej20n10gvmzr.jpg" alt=""></p><p>现有问题有：</p><ol><li>现有的VQA系统太依赖于问题而不是图片内容，并且语言的偏差会严重影响VQA系统性能。<ol><li>只需要问题或者图片就能猜出来答案，甚至一个差的数据集(通常包含具有偏差的问题)会降低VQA系统的性能。也即越具体的问题越好！[do-&gt;play-&gt;sport play]</li></ol></li><li>算法性能的提升是否真的来自于注意力机制？<ol><li>通过多全局图片特征（预训练的VGG-19,ResNet-101）也能达到很好的效果。</li><li>注意力机制有时候会误导VQA系统。</li></ol></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>可以回答任意关于图片的问题的算法将会是人工智能的里程碑。</p><h4 id="研究方向潜力股"><a href="#研究方向潜力股" class="headerlink" title="研究方向潜力股"></a>研究方向潜力股</h4><ol><li>更<strong>大</strong>更<strong>无偏</strong>更<strong>丰富</strong>的数据集:每个问题权重不应该一样；问题的质量应该更高；答案不应该是二元的；多选题应当被淘汰</li><li>更加巧妙地模型评估方式</li><li>重点：可以对图片内容进行<strong>推理</strong>的算法！<ol><li>常识推理。</li><li>空间位置。</li><li>根据不同粒度回复问题。</li></ol></li></ol><!-- TODO: ### 批注版论文 > 1. 黄色表示研究领域的问题> 2. 紫色表示论文叙述内容的重点> 3. 绿色表示该论文的解决思路> 4. 蓝色表示该论文的公式以及定义 --><h3 id="引用与参考"><a href="#引用与参考" class="headerlink" title="引用与参考"></a>引用与参考</h3><!--Ex:1. https://www.paperweekly.site/papers/notes/2212. https://scholar.google.com/-->]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;!-- 论文基本信息：方便查阅和追踪 --&gt;
&lt;!-- 论文基本信息的获取：
1. 直接从论文pdf中获取
2. 从paperweekly首页上方
      
    
    </summary>
    
      <category term="VQA" scheme="https://bupt.github.io/awesome-chatbot/categories/VQA/"/>
    
    
      <category term="summarize" scheme="https://bupt.github.io/awesome-chatbot/tags/summarize/"/>
    
  </entry>
  
  <entry>
    <title>对话AI的论文列表</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/08/09/convAI-paper-list/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/08/09/convAI-paper-list/</id>
    <published>2018-08-09T04:31:31.000Z</published>
    <updated>2018-09-04T06:19:31.740Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><blockquote><p>论文列表格式<br>&emsp;论文发表年份： 论文题目&amp;论文链接：第一作者（第一作者所属学校/机构），代码链接</p></blockquote><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h3 id="Existing-Models-of-Dialog-System"><a href="#Existing-Models-of-Dialog-System" class="headerlink" title="Existing Models of Dialog System"></a>Existing Models of Dialog System</h3><h4 id="Task-Oriented-Dialog"><a href="#Task-Oriented-Dialog" class="headerlink" title="Task-Oriented Dialog"></a>Task-Oriented Dialog</h4><ul><li>13: <a href="https://ieeexplore.ieee.org/document/6407655/" target="_blank" rel="noopener"><strong>POMDP-Based Statistical Spoken Dialog Systems: A Review</strong></a>: Steve Young(Cambridge University)</li><li>11: <a href="https://www.wiley.com/en-us/Spoken+Language+Understanding:+Systems+for+Extracting+Semantic+Information+from+Speech-p-9780470688243" target="_blank" rel="noopener"><strong>Spoken Language Understanding: Systems for Extracting Semantic Information from Speech</strong></a>: Book!</li><li>11:<a href="http://www.aclweb.org/anthology/D11-1054" target="_blank" rel="noopener"><strong>Data-Driven Response Generation in Social Media</strong></a>: Alan Ritter(University of Washington Seattle)</li><li><p>15: <a href="https://www.aclweb.org/anthology/N/N15/N15-1020.pdf" target="_blank" rel="noopener"><strong>A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</strong></a>: Alessandro Sordoni(Universite de Montreal)</p></li><li><p>15: <a href="https://arxiv.org/pdf/1506.05869.pdf" target="_blank" rel="noopener"><strong>A Neural Conversational Model</strong></a>: Oriol Vinyals(Google), <a href="https://github.com/Conchylicultor/DeepQA" target="_blank" rel="noopener"><strong>code</strong></a> via tensorflow</p></li><li>15: <a href="https://www.aclweb.org/anthology/P15-1152" target="_blank" rel="noopener"><strong>Neural Responding Machine for Short-Text Conversation</strong></a>: Lifeng Shang(Noah’s Ark Lab), <a href="https://github.com/stamdlee/DeepLearningFramework" target="_blank" rel="noopener"><strong>code</strong></a> via theano and tensorflow</li></ul><h3 id="Traditional-NLP-component-stack"><a href="#Traditional-NLP-component-stack" class="headerlink" title="Traditional NLP component stack"></a>Traditional NLP component stack</h3><h4 id="Challenge-of-NLP"><a href="#Challenge-of-NLP" class="headerlink" title="Challenge of NLP"></a>Challenge of NLP</h4><ul><li>09: <a href="https://www.cs.colorado.edu/~martin/slp.html" target="_blank" rel="noopener"><strong>SPEECH and LANGUAGE PROCESSING An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition Second Edition</strong></a>: book </li></ul><h3 id="Deep-Semantic-Similarity-Model-DSSM"><a href="#Deep-Semantic-Similarity-Model-DSSM" class="headerlink" title="Deep Semantic Similarity Model(DSSM)"></a>Deep Semantic Similarity Model(DSSM)</h3><h4 id="application-scenarios"><a href="#application-scenarios" class="headerlink" title="application scenarios"></a>application scenarios</h4><ol><li>Web search<ul><li>13: <a href="http://dl.acm.org/citation.cfm?id=2505665" target="_blank" rel="noopener"><strong>Learning deep structured semantic models for web search using clickthrough data</strong></a>: Po-Sen Huang(University of Illinois at Urbana-Champaign), <a href="https://github.com/wangtianqi1993/DL-WebSearch" target="_blank" rel="noopener"><strong>code</strong></a> via tensorflow</li><li>14: <a href="http://dl.acm.org/citation.cfm?doid=2661829.2661935" target="_blank" rel="noopener"><strong>A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval</strong></a>: Yelong Shen(Microsoft Research)</li><li>16: <a href="https://arxiv.org/abs/1502.06922" target="_blank" rel="noopener"><strong>Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval</strong></a>: Hamid Palangi, <a href="https://github.com/zhaosm/dssm-lstm" target="_blank" rel="noopener"><strong>code</strong></a></li></ul></li><li>Entity linking<ul><li>14: <a href="http://anthology.aclweb.org/D/D14/D14-1002.pdf" target="_blank" rel="noopener"><strong>Modeling Interestingness with Deep Neural Networks</strong></a>: Jianfeng Gao(Microsoft Research)</li></ul></li><li>Image captioning<ul><li>15: <a href="https://arxiv.org/abs/1411.4952" target="_blank" rel="noopener"><strong>From Captions to Visual Concepts and Back</strong></a>: Hao Fang&amp;Li Deng(Microsoft Research)</li></ul></li><li>Machine Translation<ul><li><a href="http://aclweb.org/anthology/P/P14/P14-1066.pdf" target="_blank" rel="noopener"><strong>Learning Continuous Phrase Representations for Translation Modeling</strong></a>: Jianfeng Gao(Microsoft Research)</li></ul></li><li>Online recommendation<ul><li>[<strong>duplicate</strong>] 14: <a href="http://anthology.aclweb.org/D/D14/D14-1002.pdf" target="_blank" rel="noopener"><strong>Modeling Interestingness with Deep Neural Networks</strong></a>: Jianfneg Gao(Microsoft Research)</li></ul></li></ol><h4 id="Framework-of-Model"><a href="#Framework-of-Model" class="headerlink" title="Framework of Model"></a>Framework of Model</h4><ul><li>[<strong>duplicate</strong>] 13: <a href="http://dl.acm.org/citation.cfm?id=2505665" target="_blank" rel="noopener"><strong>Learning deep structured semantic models for web search using clickthrough data</strong></a>: Po-Sen Huang(University of Illinois at Urbana-Champaign), [<strong>code</strong>]</li><li>[<strong>duplicate</strong>] 14: <a href="http://dl.acm.org/citation.cfm?doid=2661829.2661935" target="_blank" rel="noopener"><strong>A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval</strong></a>: Yelong Shen(Microsoft Research)</li><li>16: <a href="https://arxiv.org/abs/1502.06922" target="_blank" rel="noopener"><strong>Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval</strong></a>: Hamid Palangi, <a href="https://github.com/zhaosm/dssm-lstm" target="_blank" rel="noopener"><strong>code</strong></a></li><li><a href="http://aka.ms/sent2vec" target="_blank" rel="noopener">Sent2Vec</a>: software by microsoft</li></ul><h4 id="Go-beyound-DSSM"><a href="#Go-beyound-DSSM" class="headerlink" title="Go beyound DSSM"></a>Go beyound DSSM</h4><ul><li>[<strong>duplicate</strong>] 15: <a href="https://arxiv.org/abs/1411.4952" target="_blank" rel="noopener"><strong>From Captions to Visual Concepts and Back</strong></a>: Hao Fang&amp;Li Deng(Microsoft Research)</li></ul><hr><h2 id="Question-answeriing-QA-and-Machine-Readiing-Comprehension-MRC"><a href="#Question-answeriing-QA-and-Machine-Readiing-Comprehension-MRC" class="headerlink" title="Question answeriing(QA) and Machine Readiing Comprehension(MRC)"></a>Question answeriing(QA) and Machine Readiing Comprehension(MRC)</h2><h3 id="Open-Domain-Question-Answering"><a href="#Open-Domain-Question-Answering" class="headerlink" title="Open-Domain Question Answering"></a>Open-Domain Question Answering</h3><h4 id="Knowledge-Base-QA"><a href="#Knowledge-Base-QA" class="headerlink" title="Knowledge Base-QA"></a>Knowledge Base-QA</h4><ol><li>Symbolic approach via Large-scale knowledge graphs<ul><li>[<strong>oral</strong>] 98: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/COLING-98-richardson-dolan-vanderwende.pdf" target="_blank" rel="noopener">MindNet: acquiring and structuring semantic information from text</a>: Stephen D.Richardson(Microsoft Research)</li><li>[<strong>oral</strong>] 13: <a href="http://www.aclweb.org/anthology/D13-1160" target="_blank" rel="noopener">Semantic Parsing on Freebase from Question-Answer Pairs</a>: Jonathan Berant(Stanford University)</li><li>15: <a href="https://arxiv.org/pdf/1510.08565.pdf" target="_blank" rel="noopener">Attention with Intention for a Neural Network Conversation Model</a>: Kaisheng Yao(Microsoft Research)</li><li>14: <a href="http://www.aclweb.org/anthology/P14-1091" target="_blank" rel="noopener">Knowledge-Based Question Answering as Machine Translation</a>: Junwei Bao(Harbin Institute of Technology)</li><li>15: <a href="http://aclweb.org/anthology/P15-1128" target="_blank" rel="noopener">Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base</a>:Wen-tau Yih(Microsoft Research)</li></ul></li><li><p><strong>ReasoNet</strong> with Shared Memory</p><ul><li>[<strong>oral</strong>][<strong>duplicate</strong>] 16: <a href="https://arxiv.org/pdf/1611.04642.pdf?" target="_blank" rel="noopener">Link Prediction using Embedded Knowledge Graphs</a>: Yulong Shen（Microsoft&amp;Google Research）</li><li>17: <a href="https://arxiv.org/pdf/1609.05284.pdf" target="_blank" rel="noopener">ReasoNet: Learning to Stop Reading in Machine Comprehension</a>:Yelong Shen(Microsoft Research)</li></ul></li><li><p>Search Controller in <strong>ReasoNet</strong> </p><ul><li>[<strong>duplicate</strong>] 16: <a href="https://arxiv.org/pdf/1611.04642.pdf?" target="_blank" rel="noopener">Link Prediction using Embedded Knowledge Graphs</a>: Yulong Shen（Microsoft&amp;Google Research）</li></ul></li><li><strong>ReasoNet</strong> in symbolic vs neural space<ul><li>Symbolic is comprehensible but not robust<ul><li>11: <a href="http://www.cs.cmu.edu/~tom/pubs/lao-emnlp11.pdf" target="_blank" rel="noopener">Random Walk Inference and Learning in A Large Scale Knowledge Base</a>:Ni Lao(Carnegie Mellon University)</li><li>98: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/COLING-98-richardson-dolan-vanderwende.pdf" target="_blank" rel="noopener">MindNet: acquiring and structuring semantic information from text</a>:Stephen D.Richardson(Microsoft Research)</li></ul></li><li>Neural is robust but not comprehensible<ul><li>[<strong>duplicate</strong>] 16: <a href="https://arxiv.org/pdf/1611.04642.pdf?" target="_blank" rel="noopener">Link Prediction using Embedded Knowledge Graphs</a>: Yulong Shen（Microsoft&amp;Google Research）</li><li>[<strong>oral</strong>] 15: <a href="https://arxiv.org/abs/1412.6575" target="_blank" rel="noopener">EMBEDDING ENTITIES AND RELATIONS FOR LEARNING AND INFERENCE IN KNOWLEDGE BASES</a>:Bishan Yang(Cornell University), <a href="https://github.com/thunlp/OpenKE/blob/master/models/DistMult.py" target="_blank" rel="noopener">TensorFlow code</a>, <a href="https://github.com/thunlp/OpenKE/blob/OpenKE-PyTorch/models/DistMult.py" target="_blank" rel="noopener">PyTorch code</a></li></ul></li><li>Hybrid is robust and  comprehensible<ul><li>18: <a href="https://arxiv.org/pdf/1802.04394.pdf" target="_blank" rel="noopener">M-Walk: Learning to Walk in Graph with Monte Carlo Tree Search</a>:Yelong Shen(Microsoft Research&amp;Tecent AI Lab)</li><li>18: [<strong>oral</strong>] <a href="https://arxiv.org/abs/1707.06690" target="_blank" rel="noopener">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</a>:Wenhan Xiong(University of California,Santa Barbara), <a href="https://github.com/xwhan/DeepPath" target="_blank" rel="noopener">code1</a> <a href="https://github.com/arunarn2/DeepPathwithTensorforce" target="_blank" rel="noopener">code2</a></li><li>18: <a href="https://arxiv.org/abs/1711.05851" target="_blank" rel="noopener">GO FOR A WALK AND ARRIVE AT THE ANSWER: REASONING OVER PATHS IN KNOWLEDGE BASES USING REINFORCEMENT LEARNING</a>:Rajarshi Das(University of Massachusetts,Amherst), </li></ul></li></ul></li><li>Multi-turn KB-QA<ul><li><del>Programmed Dialogue policy</del><ul><li><del>15: <a href="https://arxiv.org/pdf/1504.07182.pdf" target="_blank" rel="noopener">A Probabilistic Framework for Representing Dialog Systems and Entropy-Based Dialog Management through Dynamic Stochastic State Evolution</a>:Ji Wu(IEEE)</del></li></ul></li><li>Trained via RL Dialogue policy<ul><li>16: <a href="https://arxiv.org/abs/1512.01337" target="_blank" rel="noopener">Neural Generative Question Answering </a>:Jun Yin(Noah’s Ark Lab, Huawe) <a href="https://github.com/jxfeb/Generative_QA" target="_blank" rel="noopener">corpus</a></li><li>[<strong>oral</strong>] 16: <a href="https://arxiv.org/abs/1604.04562" target="_blank" rel="noopener">A Network-based End-to-End Trainable Task-oriented Dialogue System</a>:Tsung-Hsien Wen(Cambridge University), <a href="https://github.com/shawnwun/NNDIAL" target="_blank" rel="noopener">Theano code</a></li><li>[<strong>oral</strong>] 17: <a href="https://arxiv.org/abs/1609.00777" target="_blank" rel="noopener">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</a>:Bhuwan Dhingra(Carnegie Mellon University), <a href="https://github.com/MiuLab/KB-InfoBot" target="_blank" rel="noopener">Theano code</a></li></ul></li></ul></li></ol><h4 id="Text-QA"><a href="#Text-QA" class="headerlink" title="Text-QA"></a>Text-QA</h4><ol><li>MS MARCO<ul><li>16: <a href="https://arxiv.org/abs/1611.09268" target="_blank" rel="noopener">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</a>:Tri Nguyan(Microsoft AI&amp;Research)</li></ul></li><li>SQuAD<ul><li>16: <a href="https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf" target="_blank" rel="noopener">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>:Pranav Rajpurkar(Stanford University)</li></ul></li></ol><h3 id="Neural-MRC-Models"><a href="#Neural-MRC-Models" class="headerlink" title="Neural MRC Models"></a>Neural MRC Models</h3><h4 id="BiDAF"><a href="#BiDAF" class="headerlink" title="BiDAF"></a>BiDAF</h4><ul><li>16: <a href="https://arxiv.org/pdf/1611.01603.pdf" target="_blank" rel="noopener">BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION</a>:Minjoon Seo(University of Washington)<ul><li><a href="https://github.com/imraviagrawal/ReadingComprehension" target="_blank" rel="noopener">code1</a></li><li><a href="https://github.com/bentrevett/bidaf" target="_blank" rel="noopener">code2</a> </li><li><a href="https://github.com/akhil-vader/MachineComprehension_SQuAD" target="_blank" rel="noopener">code3</a> </li><li><a href="https://github.com/RamkishanPanthena/Machine-Comprehension-using-SQuAD-Dataset" target="_blank" rel="noopener">code4</a></li></ul></li></ul><h4 id="SAN"><a href="#SAN" class="headerlink" title="SAN"></a>SAN</h4><ul><li>18: <a href="https://arxiv.org/pdf/1712.03556.pdf" target="_blank" rel="noopener">Stochastic Answer Networks for Machine Reading Comprehension</a>: Xiaodong Liu(Microsoft Research,Redmond), <a href="https://github.com/kevinduh/san_mrc" target="_blank" rel="noopener">code</a></li></ul><h4 id="Neural-MRC-Models-on-SQuAD"><a href="#Neural-MRC-Models-on-SQuAD" class="headerlink" title="Neural MRC Models on SQuAD"></a><strong>Neural MRC Models on SQuAD</strong></h4><ol><li><p>Encoding: map each text span to a semantic vector</p><ul><li>Word Embedding<ul><li>14: <a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a>:Jeffrey Pennington(Stanford University)<ul><li><a href="https://github.com/brangerbriz/midi-glove" target="_blank" rel="noopener">code:midi-glove</a></li><li><a href="https://github.com/fdurant/wiki_glove" target="_blank" rel="noopener">code:wiki-glove</a></li></ul></li><li>13: <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a>:Tomas Mikolov(Google Inc.)<ul><li><a href="https://github.com/brijml/mikolov_word2vec" target="_blank" rel="noopener">code1</a></li><li><a href="https://github.com/shuuchen/keras_word2vec" target="_blank" rel="noopener">code2</a></li></ul></li></ul></li><li><p>Context Embedding</p><ol><li><p>capture context info for each word</p><ul><li>16: <a href="http://aclweb.org/anthology/K16-1006" target="_blank" rel="noopener">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a>:Oren Melamud(Bar-Ilan University)</li><li>18: <a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a>:Matthew E.Peters(Allen Institute for Artificial Intelligence), <a href="https://github.com/zqhZY/ner_elmo" target="_blank" rel="noopener">code</a></li><li>18: <a href="https://arxiv.org/pdf/1804.09541.pdf" target="_blank" rel="noopener">QANET: COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF-ATTENTION FOR READING COMPREHENSION</a>:Adams Wei Yu(CMU&amp;Google Brain)<ul><li><a href="https://github.com/ni9elf/QANet" target="_blank" rel="noopener">code1</a></li><li><a href="https://github.com/BangLiu/QANet-PyTorch" target="_blank" rel="noopener">code2</a></li></ul></li></ul></li><li><p>Context Embedding via BiLSTM/ELmo</p><ul><li>[<strong>duplicate</strong>] 18: <a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a>:Matthew E.Peters(Allen Institute for Artificial Intelligence), <a href="https://github.com/zqhZY/ner_elmo" target="_blank" rel="noopener">code</a></li><li>17: <a href="https://arxiv.org/abs/1708.00107" target="_blank" rel="noopener">Learned in Translation: Contextualized Word Vectors</a>:Bryan McCann(SalesForce)</li><li>16: [duplicate]<a href="http://aclweb.org/anthology/K16-1006" target="_blank" rel="noopener">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a>:Oren Melamud(Bar-Ilan University)</li></ul></li><li><p>Context Embedding</p><ul><li>[<strong>duplicate</strong>] 18: <a href="https://arxiv.org/pdf/1804.09541.pdf" target="_blank" rel="noopener">QANET: COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF-ATTENTION FOR READING COMPREHENSION</a>:Adams Wei Yu(CMU&amp;Google Brain)<ul><li><a href="https://github.com/ni9elf/QANet" target="_blank" rel="noopener">code1</a></li><li><a href="https://github.com/BangLiu/QANet-PyTorch" target="_blank" rel="noopener">code2</a></li></ul></li></ul></li></ol></li></ul><ul><li>Query-context/Content-query attention</li></ul></li><li><p>Reasoning: rank and re-rank semantic vectors</p><ul><li><p>Multi-step reasoning for Text-QA</p><ul><li>[<strong>duplicate</strong>] 17: <a href="https://arxiv.org/pdf/1609.05284.pdf" target="_blank" rel="noopener">ReasoNet: Learning to Stop Reading in Machine Comprehension</a>:Yelong Shen(Microsoft Research)</li></ul></li><li><p>Stochastic Answer Net</p><ul><li>[<strong>duplicate</strong>] 18: <a href="https://arxiv.org/pdf/1712.03556.pdf" target="_blank" rel="noopener">Stochastic Answer Networks for Machine Reading Comprehension</a>: Xiaodong Liu(Microsoft Research,Redmond), <a href="https://github.com/kevinduh/san_mrc" target="_blank" rel="noopener">code</a></li></ul></li></ul></li></ol><hr><h2 id="Task-oriented-dialogues"><a href="#Task-oriented-dialogues" class="headerlink" title="Task-oriented dialogues"></a>Task-oriented dialogues</h2><h3 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h3><h4 id="A-Example-Dialogue-with-Movie-Bot"><a href="#A-Example-Dialogue-with-Movie-Bot" class="headerlink" title="A Example Dialogue with Movie-Bot"></a>A Example Dialogue with Movie-Bot</h4><ul><li><a href="https://github.com/MiuLab/TC-Bot" target="_blank" rel="noopener">source code</a></li></ul><h4 id="Conversation-as-Reinforcement-Learning"><a href="#Conversation-as-Reinforcement-Learning" class="headerlink" title="Conversation as Reinforcement Learning"></a>Conversation as Reinforcement Learning</h4><ul><li>00: <a href="http://www.thepieraccinis.com/publications/2000/IEEE_TSAP_00.pdf" target="_blank" rel="noopener">A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies</a>: Esther Levin(IEEE)</li><li>00: <a href="https://web.eecs.umich.edu/~baveja/Papers/RLDSjair.pdf" target="_blank" rel="noopener">Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System</a>:Satinder Singh(AT&amp;T Labs)</li><li>07: <a href="http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf" target="_blank" rel="noopener">Partially observable Markov decision processes for spoken dialog systems</a>:Jason D.Williams(AT&amp;T Labs)</li></ul><h4 id="Dialogue-System-Evaluation-Simulated-Users"><a href="#Dialogue-System-Evaluation-Simulated-Users" class="headerlink" title="Dialogue System Evaluation(Simulated Users)"></a>Dialogue System Evaluation(Simulated Users)</h4><ol><li>Agenda based<ul><li>09: <a href="https://ieeexplore.ieee.org/document/4806280/" target="_blank" rel="noopener">The Hidden Agenda User Simulation Model</a>:Jost Schatzmann(IEEE)</li><li><a href="https://github.com/MiuLab/TC-Bot" target="_blank" rel="noopener">source code</a> </li></ul></li><li>Model based<ul><li>16: <a href="https://arxiv.org/abs/1607.00070" target="_blank" rel="noopener">A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems</a>: Layla El Asri(Maluuba Research)</li><li>17: <a href="https://arxiv.org/pdf/1703.01008.pdf" target="_blank" rel="noopener">End-to-End Task-Completion Neural Dialogue Systems</a>:Xiujun Li(Microsoft Research&amp;National Taiwan University)</li></ul></li></ol><h3 id="traditional-approache"><a href="#traditional-approache" class="headerlink" title="traditional approache"></a>traditional approache</h3><h4 id="Decison-theoretic-View-of-Dialogue-Management"><a href="#Decison-theoretic-View-of-Dialogue-Management" class="headerlink" title="Decison-theoretic View of Dialogue Management"></a>Decison-theoretic View of Dialogue Management</h4><ul><li>[<strong>duplicate</strong>] 00: <a href="https://web.eecs.umich.edu/~baveja/Papers/RLDSjair.pdf" target="_blank" rel="noopener">Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System</a>:Satinder Singh(AT&amp;T Labs)</li><li>00: <a href="http://www.thepieraccinis.com/publications/2000/IEEE_TSAP_00.pdf" target="_blank" rel="noopener">A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies</a>: Esther Levin(IEEE)</li><li>00: <a href="http://www.aclweb.org/anthology/P98-2219" target="_blank" rel="noopener">Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email</a>: Marilyn A.Walker(ATT Labs Research)</li><li>02: <a href="https://dl.acm.org/citation.cfm?id=1289246" target="_blank" rel="noopener">Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning</a>:Konrad Scheffler(Cambridge University)</li></ul><h4 id="Language-Understanding-Uncertainty-POMDP-as-a-principled-framework"><a href="#Language-Understanding-Uncertainty-POMDP-as-a-principled-framework" class="headerlink" title="Language Understanding Uncertainty: POMDP as a principled framework"></a>Language Understanding Uncertainty: POMDP as a principled framework</h4><ul><li>00: <a href="http://www.mit.edu/~nickroy/papers/acl00.pdf" target="_blank" rel="noopener">Spoken Dialogue Management Using Probabilistic Reasoning</a>: Nicholas Roy(Carnegie Mellon University)</li><li>01: <a href="http://www.wytsg.org:88/reslib/400/180/110/020/010/130/L000000000233767.pdf" target="_blank" rel="noopener">Spoken Dialogue Management as Planning and Acting under Uncertainty</a>:Bo Zhang(Tech. of China)</li><li>07: <a href="http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf" target="_blank" rel="noopener">Partially observable Markov decision processes for spoken dialog systems</a>:Jason D.Williams(AT&amp;T Labs)</li></ul><h4 id="scaling-up-Dialogue-Optimization"><a href="#scaling-up-Dialogue-Optimization" class="headerlink" title="scaling up Dialogue Optimization"></a>scaling up Dialogue Optimization</h4><ol><li>Use approxmiate POMDP algorithms leveraging problem-specific structure<ul><li>00: <a href="http://www.mit.edu/~nickroy/papers/acl00.pdf" target="_blank" rel="noopener">Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning</a>:Konrad Scheffler(Cambridge University)</li><li>07: <a href="http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf" target="_blank" rel="noopener">Partially observable Markov decision processes for spoken dialog systems</a>:Jason D.Williams(AT&amp;T Labs)</li></ul></li><li>Use Reinforcement Learning algorithms with function approximation<ul><li>08: <a href="http://www.aclweb.org/anthology/J08-4002" target="_blank" rel="noopener">Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets</a>: James Henderson</li><li>09: <a href="https://pdfs.semanticscholar.org/a950/d7836e101e7d649791714d8383a804a6f671.pdf" target="_blank" rel="noopener">Reinforcement Learning for Dialog Management using Least-Squares Policy Iteration and Fast Feature Selection</a>: Lihong Li(Rutgers University)</li><li>14: <a href="http://mi.eng.cam.ac.uk/~sjy/papers/gktb14.pdf" target="_blank" rel="noopener">Incremental on-line adaptation of POMDP-based dialogue managers to extended domains</a>:M.Gasic[Cambridge University]</li></ul></li></ol><h3 id="Natural-language-understanding-and-dialogue-state-tracking"><a href="#Natural-language-understanding-and-dialogue-state-tracking" class="headerlink" title="Natural language understanding and dialogue state tracking"></a>Natural language understanding and dialogue state tracking</h3><h4 id="Language-Understanding"><a href="#Language-Understanding" class="headerlink" title="Language Understanding"></a>Language Understanding</h4><ol><li><p>DNN for Domain/Intent Classification</p><ul><li>15:  <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/RNNLM_addressee.pdf" target="_blank" rel="noopener">Recurrent Neural Network and LSTM Models for Lexical Utterance Classification</a>: Suman Raviuri(University of California,Berkeley)</li></ul></li><li><p>Slot filling</p><ul><li>16: <a href="https://www.csie.ntu.edu.tw/~yvchen/doc/IS16_MultiJoint.pdf" target="_blank" rel="noopener">Multi-Domain Joint Semantic Frame Parsing using Bi-directional RNN-LSTM</a>: Dilek Hakkani-Tur(Microsoft Research)</li></ul></li><li><p>Further details on NLU</p><ul><li><a href="https://www.csie.ntu.edu.tw/~yvchen/doc/OpenDialogue_Tutorial_IJCNLP.pdf" target="_blank" rel="noopener">ppt</a></li><li>E2E MemNN for Contectual LU: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/IS16_ContextualSLU.pdf" target="_blank" rel="noopener">End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding</a>: Yun-Nung Chen(National Taiwan University )</li><li>[<strong>duplicate</strong>] LU Importance: 17: <a href="https://arxiv.org/pdf/1703.01008.pdf" target="_blank" rel="noopener">End-to-End Task-Completion Neural Dialogue Systems</a>:Xiujun Li(Microsoft Research&amp;National Taiwan University)</li></ul></li></ol><h4 id="Dialogue-State-Tracking-DST"><a href="#Dialogue-State-Tracking-DST" class="headerlink" title="Dialogue State Tracking(DST)"></a>Dialogue State Tracking(DST)</h4><ol><li>DSTC(Dialog State Tracking Challenge)<ul><li><a href="https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/" target="_blank" rel="noopener">DSTC1 official website</a></li><li><a href="http://camdial.org/~mh521/dstc/" target="_blank" rel="noopener">DSTC2&amp;3 official website</a></li><li><a href="http://www.colips.org/workshop/dstc4/" target="_blank" rel="noopener">DSTC4 official website</a></li><li><a href="http://workshop.colips.org/dstc5/" target="_blank" rel="noopener">DSTC5 official website</a></li></ul></li><li><p>Neural Belief Tracker</p><ul><li>16: <a href="https://arxiv.org/abs/1606.03777" target="_blank" rel="noopener">Neural Belief Tracker: Data-Driven Dialogue State Tracking</a>: Nikola Mrksic(University of Cambridge)</li></ul></li><li><p>NN-Based DST</p><ul><li>13: <a href="http://www.anthology.aclweb.org/W/W13/W13-4073.pdf" target="_blank" rel="noopener">Deep Neural Network Approach for the Dialog State Tracking Challenge</a>: Matthew Henderson(University of Cambridge)</li><li>15: <a href="https://arxiv.org/abs/1506.07190" target="_blank" rel="noopener">Multi-domain Dialog State Tracking using Recurrent Neural Networks</a>: Nikola Mrksic(University of Cambridge)</li><li>[<strong>duplicate</strong>] 16: <a href="https://arxiv.org/abs/1606.03777" target="_blank" rel="noopener">Neural Belief Tracker: Data-Driven Dialogue State Tracking</a>: Nikola Mrksic(University of Cambridge)</li></ul></li></ol><h3 id="Deep-RL-for-dialogue-policy-learning"><a href="#Deep-RL-for-dialogue-policy-learning" class="headerlink" title="Deep RL for dialogue policy learning"></a>Deep RL for dialogue policy learning</h3><h4 id="Two-main-classed-of-RL-algorithms"><a href="#Two-main-classed-of-RL-algorithms" class="headerlink" title="Two main classed of RL algorithms"></a>Two main classed of RL algorithms</h4><ol><li>Value function based:<ul><li>15: <a href="https://www.nature.com/articles/nature14236" target="_blank" rel="noopener">Human-level control through deep reinforcement learning</a>: Volodymyr Minh<ul><li><a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">code1</a> by tensorflow</li><li><a href="https://github.com/pianomania/DQN-pytorch" target="_blank" rel="noopener">code2</a> by pytorch</li></ul></li><li>16: <a href="https://arxiv.org/pdf/1606.02560.pdf" target="_blank" rel="noopener">Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning</a>: Tiancheng Zhao(Carnegie Mellon University)</li></ul></li><li>Policy based:<ul><li>92: <a href="https://doi.org/10.1007/BF00992696" target="_blank" rel="noopener">Simple statistical gradient-following algorithms for connectionist reinforcement learning</a>: Ronald J.Williams</li><li>17: <a href="http://www.aclweb.org/anthology/P/P16/P16-1230.pdf" target="_blank" rel="noopener">On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems</a>: Pei-Hao Su(University of Cambridge)</li></ul></li></ol><h4 id="Domain-Extension-and-Exploration-BBQ-network"><a href="#Domain-Extension-and-Exploration-BBQ-network" class="headerlink" title="Domain Extension and Exploration(BBQ network)"></a>Domain Extension and Exploration(BBQ network)</h4><ul><li>18: <a href="https://arxiv.org/pdf/1608.05081.pdf" target="_blank" rel="noopener">BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems</a>: Zachary Lipton(Carnegir Mellon University)</li></ul><h4 id="Composite-task-Dialogues"><a href="#Composite-task-Dialogues" class="headerlink" title="Composite-task Dialogues"></a>Composite-task Dialogues</h4><ol><li>A Hierarchical Policy Learner<ul><li>98: <a href="http://papers.nips.cc/paper/1384-reinforcement-learning-with-hierarchies-of-machines.pdf" target="_blank" rel="noopener">Reinforcement Learning with Hierarchies of Machines</a>: Ronald Parr(UC Berkeley)</li><li>17: <a href="https://arxiv.org/abs/1704.03084" target="_blank" rel="noopener">Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning</a>: Baolin Peng(Microsoft Research)</li></ul></li><li>Integrating Planning for Dialogue Policy Learning<ul><li>18: <a href="https://arxiv.org/abs/1801.06176" target="_blank" rel="noopener">Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning</a>: Baolin Peng(Microsoft Research) , <a href="https://github.com/MiuLab/DDQ" target="_blank" rel="noopener">code</a></li></ul></li></ol><h3 id="Decision-theoretic-View-of-Dialogue-Management"><a href="#Decision-theoretic-View-of-Dialogue-Management" class="headerlink" title="Decision-theoretic View of Dialogue Management"></a>Decision-theoretic View of Dialogue Management</h3><h4 id="Hybrid-Code-Networks"><a href="#Hybrid-Code-Networks" class="headerlink" title="Hybrid Code Networks"></a>Hybrid Code Networks</h4><ul><li>17: <a href="https://arxiv.org/abs/1702.03274" target="_blank" rel="noopener">Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning</a>: Jason D. Williams(Microsoft Research)</li></ul><h4 id="Differentiating-KB-Accesses"><a href="#Differentiating-KB-Accesses" class="headerlink" title="Differentiating KB Accesses"></a>Differentiating KB Accesses</h4><ul><li>[<strong>duplicate</strong>] 17: <a href="https://arxiv.org/abs/1609.00777" target="_blank" rel="noopener">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</a>:Bhuwan Dhingra(Carnegie Mellon University)</li></ul><h4 id="An-E2E-Neural-Dialogue-System"><a href="#An-E2E-Neural-Dialogue-System" class="headerlink" title="An E2E Neural Dialogue System"></a>An E2E Neural Dialogue System</h4><ul><li>[<strong>duplicate</strong>] 17: <a href="https://arxiv.org/pdf/1703.01008.pdf" target="_blank" rel="noopener">End-to-End Task-Completion Neural Dialogue Systems</a>:Xiujun Li(Microsoft Research&amp;National Taiwan University)</li></ul><hr><h2 id="Fully-data-driven-conversation-models-and-chatbots"><a href="#Fully-data-driven-conversation-models-and-chatbots" class="headerlink" title="Fully data-driven conversation models and chatbots"></a>Fully data-driven conversation models and chatbots</h2><h3 id="Historical-overview"><a href="#Historical-overview" class="headerlink" title="Historical overview"></a>Historical overview</h3><h4 id="Response-retrival-system"><a href="#Response-retrival-system" class="headerlink" title="Response retrival system"></a>Response retrival system</h4><ul><li>10: <a href="https://aritter.github.io/chat.pdf" target="_blank" rel="noopener">Filter, Rank, and Transfer the Knowledge: Learning to Chat</a>:<br>Alan Ritter(University of Washington)</li></ul><h4 id="Response-generation-using-Statistical-Machine-Translation"><a href="#Response-generation-using-Statistical-Machine-Translation" class="headerlink" title="Response generation using Statistical Machine Translation"></a>Response generation using Statistical Machine Translation</h4><ul><li>11:  <a href="http://www.aclweb.org/anthology/D11-1054" target="_blank" rel="noopener">Data-Driven Response Generation in Social Media</a>: Alan Ritter(University of Washington)</li></ul><h4 id="First-neural-response-generation-systems"><a href="#First-neural-response-generation-systems" class="headerlink" title="First neural response generation systems"></a>First neural response generation systems</h4><ol><li>Neural Models for Response Generation<ul><li>15: <a href="https://www.aclweb.org/anthology/N/N15/N15-1020.pdf" target="_blank" rel="noopener">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</a>: Alessandro Sordoni(University de Montreal)</li><li>15: <a href="https://arxiv.org/pdf/1506.05869.pdf" target="_blank" rel="noopener">A Neural Conversational Model</a>: Oriol Vinyals(Google .Inc)</li><li>15: <a href="https://www.aclweb.org/anthology/P15-1152" target="_blank" rel="noopener">Neural Responding Machine for Short-Text Conversation</a>: Lifeng Shang(Noah’s Ark Lab), <a href="https://github.com/stamdlee/DeepLearningFramework" target="_blank" rel="noopener">code</a></li></ul></li><li>Neural conversation engine: <ul><li>16: <a href="http://arxiv.org/abs/1510.03055" target="_blank" rel="noopener">A Diversity-Promoting Objective Function for Neural Conversation Models</a>: Jiwei Li(Stanford University)</li></ul></li></ol><h3 id="challenges-and-remedies"><a href="#challenges-and-remedies" class="headerlink" title="challenges and remedies"></a>challenges and remedies</h3><h4 id="Challenge-The-blandness-problem"><a href="#Challenge-The-blandness-problem" class="headerlink" title="Challenge: The blandness problem"></a>Challenge: The blandness problem</h4><ul><li>[<strong>duplicate</strong>] 16: <a href="http://arxiv.org/abs/1510.03055" target="_blank" rel="noopener">A Diversity-Promoting Objective Function for Neural Conversation Models</a>: Jiwei Li(Stanford University)</li></ul><h4 id="Challenge-The-consistency-problem"><a href="#Challenge-The-consistency-problem" class="headerlink" title="Challenge: The consistency problem"></a>Challenge: The consistency problem</h4><ol><li>Solution: Personalized Response Generation<ul><li>Microsoft Personality chat:speaker embedding LSTM: <a href="https://arxiv.org/abs/1603.06155" target="_blank" rel="noopener">A Persona-Based Neural Conversation Model</a>: Jiwei Li(Stanford University), <a href="https://github.com/fionn-mac/A-Persona-Based-Neural-Conversation-Model" target="_blank" rel="noopener">code</a> via Pytorch</li></ul></li><li>Personal modeling as multi-task learning<ul><li>17: <a href="https://arxiv.org/abs/1710.07388" target="_blank" rel="noopener">Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation Models</a>: Yi Luan(University of Washington)</li></ul></li><li>Improving personalization with multiple losses<ul><li>16: <a href="https://arxiv.org/pdf/1606.00372.pdf" target="_blank" rel="noopener">Conversational Contextual Cues: The Case of Personalization and History for Response Ranking</a>: Rami Al-Rfou(Google .Inc)</li></ul></li></ol><h4 id="Challenge-Long-conversational-context"><a href="#Challenge-Long-conversational-context" class="headerlink" title="Challenge: Long conversational context"></a>Challenge: Long conversational context</h4><ol><li>It can be challenging for LSTM/GRU to encode very long context<ul><li>18: <a href="https://arxiv.org/abs/1805.04623" target="_blank" rel="noopener">Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context</a>: Urvashi Khadelwal(Stanford University)</li></ul></li><li>Hierarchical Encoder-Decoder(HRED), <a href="https://github.com/urvashik/lm-context-analysis" target="_blank" rel="noopener">code</a><ul><li>16: <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11957/12160" target="_blank" rel="noopener">Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</a>: Iulian V.Serban(University de Montreal), <a href="https://github.com/hsgodhia/hred" target="_blank" rel="noopener">code</a></li></ul></li><li>Hierarchical Latent Variable Encoder-Decoder(VHRED)<ul><li>17: <a href="http://www.cs.toronto.edu/~lcharlin/papers/vhred_aaai17.pdf" target="_blank" rel="noopener">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</a>: Iulian V. Serban</li></ul></li></ol><h3 id="Grounded-conversation-models"><a href="#Grounded-conversation-models" class="headerlink" title="Grounded conversation models"></a>Grounded conversation models</h3><h4 id="A-Knowledge-Grounded-Neural-Conversation-Model"><a href="#A-Knowledge-Grounded-Neural-Conversation-Model" class="headerlink" title="A Knowledge-Grounded Neural Conversation Model"></a>A Knowledge-Grounded Neural Conversation Model</h4><ul><li>15: <a href="https://arxiv.org/pdf/1503.08895.pdf" target="_blank" rel="noopener">End-To-End Memory Networks</a>: Sainbayar Sukhbaatar(New York University)<ul><li><a href="https://github.com/carpedm20/MemN2N-tensorflow" target="_blank" rel="noopener">code1</a> via Tensorflow</li><li><a href="https://github.com/domluna/memn2n" target="_blank" rel="noopener">code2</a> via Tensorflow</li><li><a href="https://github.com/vinhkhuc/MemN2N-babi-python" target="_blank" rel="noopener">code3</a> for bAbI QA tasks</li></ul></li><li>17: <a href="https://arxiv.org/abs/1702.01932" target="_blank" rel="noopener">A Knowledge-Grounded Neural Conversation Model</a>: Marjan Gahzvininejad(USC)</li></ul><h4 id="Grounded-E2E-Dialogue-Systems"><a href="#Grounded-E2E-Dialogue-Systems" class="headerlink" title="Grounded E2E Dialogue Systems"></a>Grounded E2E Dialogue Systems</h4><ul><li>16: <a href="https://arxiv.org/abs/1611.08669" target="_blank" rel="noopener">Visual Dialog</a>: Abhishek Das(Georgia Institute of Tehhnology)<ul><li><a href="https://github.com/batra-mlp-lab/visdial" target="_blank" rel="noopener">code1</a> via Lua</li><li><a href="https://github.com/jiasenlu/visDial.pytorch" target="_blank" rel="noopener">code2</a> via Pytorch</li></ul></li><li>17: <a href="https://arxiv.org/abs/1701.08251" target="_blank" rel="noopener">Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation</a>: Nasrin Mostafazadeh(University of Rochster)</li><li>18: <a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/04/huber2018chi.small_.pdf" target="_blank" rel="noopener">Emotional Dialogue Generation using Image-Grounded Language Models</a>:Bernd Huber(Harvard University)</li></ul><h3 id="Beyond-supervised-learning-Deep-Reinforcement-Learning-for-E2E-Dialogue"><a href="#Beyond-supervised-learning-Deep-Reinforcement-Learning-for-E2E-Dialogue" class="headerlink" title="Beyond supervised learning(Deep Reinforcement Learning for E2E Dialogue)"></a>Beyond supervised learning(Deep Reinforcement Learning for E2E Dialogue)</h3><ul><li>16: <a href="https://arxiv.org/abs/1606.01541" target="_blank" rel="noopener">Deep Reinforcement Learning for Dialogue Generation</a>:Jiwei Li(Stanford University)<ul><li><a href="https://github.com/liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow" target="_blank" rel="noopener">code1</a> via Tensorflow</li><li><a href="https://github.com/agsarthak/Goal-oriented-Dialogue-Systems" target="_blank" rel="noopener">code2</a> via keras</li><li><a href="https://github.com/jiweil/Neural-Dialogue-Generation" target="_blank" rel="noopener">code3</a> by Jiwei Li</li></ul></li></ul><h3 id="Data-and-evaluation"><a href="#Data-and-evaluation" class="headerlink" title="Data and evaluation"></a>Data and evaluation</h3><h4 id="Conversational-datasets-for-social-bots-E2E-dialogue-research"><a href="#Conversational-datasets-for-social-bots-E2E-dialogue-research" class="headerlink" title="Conversational datasets(for social bots, E2E dialogue research)"></a>Conversational datasets(for social bots, E2E dialogue research)</h4><ul><li>15: <a href="https://arxiv.org/abs/1512.05742" target="_blank" rel="noopener">A Survey of Available Corpora for Building Data-Driven Dialogue Systems</a>: Iulian Vlad Serban(Universite de Montreal)</li></ul><h4 id="Evaluating-E2E-Dialogue-Systems-via-Autumatic-evaluation"><a href="#Evaluating-E2E-Dialogue-Systems-via-Autumatic-evaluation" class="headerlink" title="Evaluating E2E Dialogue Systems via Autumatic evaluation"></a>Evaluating E2E Dialogue Systems via Autumatic evaluation</h4><ol><li>Machine-Translation-Based Metric<ul><li>02: <a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener">BLEU: a Method for Automatic Evaluation of Machine Translation</a>: Kishore Papineni(IBM), <a href="https://github.com/abidasari/NLPHW4" target="_blank" rel="noopener">code</a></li><li>02: <a href="http://www.mt-archive.info/HLT-2002-Doddington.pdf" target="_blank" rel="noopener">Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics</a>: George Doddington</li></ul></li><li>Sentence-level correlation of MT metrics:<ul><li>16: <a href="https://aclweb.org/anthology/D16-1230" target="_blank" rel="noopener">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</a>: Chia-Wei Liu(McGill University)</li><li>15: <a href="http://www.aclweb.org/anthology/N15-1124" target="_blank" rel="noopener">Accurate Evaluation of Segment-level Machine Translation Metrics</a>: Yvette Graham(The University of Melbourne)</li></ul></li></ol><h4 id="The-importance-of-sample-size"><a href="#The-importance-of-sample-size" class="headerlink" title="The importance of sample size"></a>The importance of sample size</h4><ul><li>[<strong>duplicate</strong>] 02: <a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener">BLEU: a Method for Automatic Evaluation of Machine Translation</a>: Kishore Papineni(IBM), <a href="https://github.com/abidasari/NLPHW4" target="_blank" rel="noopener">code</a></li><li>06: <a href="http://homepages.inf.ed.ac.uk/pkoehn/publications/bootstrap2004.pdf" target="_blank" rel="noopener">Statistical Significance Tests for Machine Translation Evaluation</a>: Philipp Kowehn(MIT)</li></ul><h4 id="Corpus-level-Correlation"><a href="#Corpus-level-Correlation" class="headerlink" title="Corpus-level Correlation"></a>Corpus-level Correlation</h4><ul><li>[<strong>duplicate</strong>] 02: <a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener">BLEU: a Method for Automatic Evaluation of Machine Translation</a>: Kishore Papineni(IBM), <a href="https://github.com/abidasari/NLPHW4" target="_blank" rel="noopener">code</a></li><li>[<strong>duplicate</strong>] 06: <a href="http://homepages.inf.ed.ac.uk/pkoehn/publications/bootstrap2004.pdf" target="_blank" rel="noopener">Statistical Significance Tests for Machine Translation Evaluation</a>: </li></ul><h3 id="Chatbot-in-public"><a href="#Chatbot-in-public" class="headerlink" title="Chatbot in public"></a>Chatbot in public</h3><h4 id="Social-Bots-commercial-systems"><a href="#Social-Bots-commercial-systems" class="headerlink" title="Social Bots: commercial systems"></a>Social Bots: commercial systems</h4><ol><li>For end users<ul><li>Replika.ai system description: <a href="https://github.com/lukalabs/replika-research/blob/master/scai2017/replika_ai.pdf" target="_blank" rel="noopener">replika_ai</a>: Slides</li><li>XiaoIce:<br>15:<a href="https://www.nytimes.com/interactive/2015/07/27/science/chatting-with-xiaoice.html" target="_blank" rel="noopener">Chatting With Xiaoice</a>: News</li></ul></li><li>For bot developers<ul><li>[<strong>duplicate</strong>] Microsoft Personality chat:speaker embedding LSTM: <a href="https://arxiv.org/abs/1603.06155" target="_blank" rel="noopener">A Persona-Based Neural Conversation Model</a>: Jiwei Li(Stanford University), <a href="https://github.com/fionn-mac/A-Persona-Based-Neural-Conversation-Model" target="_blank" rel="noopener">code</a> via Pytorch</li><li>Microsoft Personality chat’s API: <a href="https://labs.cognitive.microsoft.com/en-us/project-personality-chat" target="_blank" rel="noopener">Project Personality Chat’s url</a> </li></ul></li></ol><h4 id="Open-Benchmarks"><a href="#Open-Benchmarks" class="headerlink" title="Open Benchmarks"></a>Open Benchmarks</h4><ol><li><p>Alexa Challenge</p><ul><li>website: <a href="https://developer.amazon.com/alexaprize/proceedings" target="_blank" rel="noopener">Alexa Prize Proceedings</a></li></ul></li><li><p>Dialogue System Technology Challenge(DSTC)</p><ul><li><a href="http://workshop.colips.org/dstc7" target="_blank" rel="noopener">DSTC7</a></li><li>Visual-Scene: <a href="https://github.com/hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge" target="_blank" rel="noopener">DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge 2018</a></li><li>background article:<br><a href="https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling" target="_blank" rel="noopener">DSTC7-End-to-End-Conversation-Modeling 2018</a></li><li>Registration Link:<br><a href="http://workshop.colips.org/dstc7/call.html" target="_blank" rel="noopener">DSTC7 Registration</a></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;blockquote&gt;
&lt;p&gt;论文列表格式&lt;br&gt;&amp;emsp;论文发表年份： 论文题目&amp;amp;论文链接：第一作者（第一作者所属学校/机构），代码
      
    
    </summary>
    
      <category term="AI" scheme="https://bupt.github.io/awesome-chatbot/categories/AI/"/>
    
    
      <category term="chatbot" scheme="https://bupt.github.io/awesome-chatbot/tags/chatbot/"/>
    
      <category term="conversationalAI" scheme="https://bupt.github.io/awesome-chatbot/tags/conversationalAI/"/>
    
      <category term="nlp" scheme="https://bupt.github.io/awesome-chatbot/tags/nlp/"/>
    
      <category term="paper" scheme="https://bupt.github.io/awesome-chatbot/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>对话AI的术语和学习地图</title>
    <link href="https://bupt.github.io/awesome-chatbot/2018/08/08/convAI-map-and-term/"/>
    <id>https://bupt.github.io/awesome-chatbot/2018/08/08/convAI-map-and-term/</id>
    <published>2018-08-08T15:25:39.000Z</published>
    <updated>2018-08-24T06:59:54.094Z</updated>
    
    <content type="html"><![CDATA[<script src="/awesome-chatbot/assets/js/APlayer.min.js"> </script><h2 id="chatbot基本篇"><a href="#chatbot基本篇" class="headerlink" title="chatbot基本篇"></a>chatbot基本篇</h2><ul><li><p>Natural language processing(自然语言处理/NLP)<br>自然语言处理是人工智能的一个子集领域。自然语言处理是一项包罗万象且相当复杂的技术，它包含许多子集，如自然语言理解。<br>NLP指的是机器理解人类输入的所有东西。为此，NLP引擎将使用许多工具，如NLU，总结算法，情绪分析，标记化等等。</p></li><li><p>Natural language understanding (自然语言理解/NLU)<br>自然语言理解是自然语言处理的一个子集。NLU和NLP经常被混淆，因为它们的意思非常接近。<br>NLU是NLP引擎中非常具体的部分，它检查话语并提取其实体和意图。用更通俗的话说，NLU允许机器理解用户在说什么。<br>说到聊天机器人，可以把NLU想象成阅读人类语言并识别文本不同部分的过程，把它分解成正确的意图和实体</p></li><li><p>Chatbot(聊天机器人)<br><code>chatbot</code>是一个可对话的计算机程序。但是<strong>对话agent</strong>可能是形容这个程序更好的词汇。</p></li><li><p>Utterance(表达)<br>用户对chatbot说的任何话，也可以看做是用户输入。例如，如果用户输入“给我看昨天的财经新闻”，整个句子就是<code>Utterance</code>。</p></li><li><p>Intent(意图))<br><code>Intent</code>代表了用户<code>Utterance</code>的意义。Chatbot将会根据用户一系列的<code>Intent</code>和对<code>Intent</code>的理解来回应用户。例如，如果用户输入“show me yesterday’s financial news”，用户的意图是检索金融标题列表。<code>Intent</code>通常是一个动词和一个名词，如“showNews”。</p></li><li><p>Entity(实体)<br><code>Entity</code>通常修饰<code>Intent</code>。例如，如果用户输入“show me yesterday’s financial news”，那么<code>Entity</code>是“yesterday”和“financial”。<code>Entity</code>会被赋予一个名称，例如“dateTime”和“newsType”。<code>Entity</code>有时也被称为<code>Slots</code>。</p></li></ul><p><img src="http://ww1.sinaimg.cn/mw690/ca26ff18ly1fu2otnnx3oj21r60x246f.jpg" alt=""></p><ul><li><p>Broadcast(广播)<br><code>Broadcast</code>是预先发送给用户的消息。它不是对用户输入的响应。<code>Broadcast</code>也被称为“订阅消息”，它相当于聊天机器人中的移动应用程序中的推送消息。</p></li><li><p>Ambiguity</p></li><li>Paraphrase</li><li>metric</li></ul><h2 id="术语进阶篇"><a href="#术语进阶篇" class="headerlink" title="术语进阶篇"></a>术语进阶篇</h2><h3 id="NLP常用术语"><a href="#NLP常用术语" class="headerlink" title="NLP常用术语"></a>NLP常用术语</h3><h4 id="词级别"><a href="#词级别" class="headerlink" title="词级别"></a>词级别</h4><ul><li>分词（Seg）</li><li>词性标注（POS）</li><li>命名实体识别（NER）</li><li>未登录词识别</li><li>词向量（word2vec）</li><li>词义消歧</li></ul><h4 id="句子级别"><a href="#句子级别" class="headerlink" title="句子级别"></a>句子级别</h4><ul><li>情感分析</li><li>关系提取</li><li>意图识别</li><li>依存句法分析（parser）</li><li>角色标注，</li><li>浅层语义分析，</li><li>指代消解</li></ul><h4 id="篇章级别"><a href="#篇章级别" class="headerlink" title="篇章级别"></a>篇章级别</h4><ul><li>信息抽取：</li><li>本体提取：</li><li>事件抽取：</li><li>主题提取：</li><li>文档聚类：</li><li>舆情分析：</li><li>篇章理解：</li><li>自动文摘：</li></ul><h4 id="常用基础算法："><a href="#常用基础算法：" class="headerlink" title="常用基础算法："></a>常用基础算法：</h4><ol><li><p>机器学习：</p><ul><li>隐马尔科夫（HMM）</li><li>条件随机场（CRF）</li><li>支持向量机（SVM）</li><li>语言模型</li><li>主题模型（LDA）</li><li>TF-IDF</li><li>互信息（PMI）</li><li>贝叶斯模型</li><li>概率图模型   </li></ul></li><li><p>深度学习:</p></li></ol><h3 id="Qustion-Answering-QA"><a href="#Qustion-Answering-QA" class="headerlink" title="Qustion Answering(QA)"></a>Qustion Answering(QA)</h3><h3 id="Reinfoecement-Learning-强化学习-RL"><a href="#Reinfoecement-Learning-强化学习-RL" class="headerlink" title="Reinfoecement Learning(强化学习/RL)"></a>Reinfoecement Learning(强化学习/RL)</h3><h3 id="Markov-Decision-Process-马尔科夫决策过程-MDP"><a href="#Markov-Decision-Process-马尔科夫决策过程-MDP" class="headerlink" title="Markov Decision Process(马尔科夫决策过程/MDP)"></a>Markov Decision Process(马尔科夫决策过程/MDP)</h3><h3 id="POMDP"><a href="#POMDP" class="headerlink" title="POMDP"></a>POMDP</h3><h3 id="Image-captioning"><a href="#Image-captioning" class="headerlink" title="Image captioning"></a>Image captioning</h3><h3 id="Phonology"><a href="#Phonology" class="headerlink" title="Phonology"></a>Phonology</h3><h3 id="分词（Segment）"><a href="#分词（Segment）" class="headerlink" title="分词（Segment）"></a>分词（Segment）</h3><p>中英文都存在分词的问题，不过相对来说，英文单词与单词之间本来就有空格进行分割，所以处理起来相对方便。但是中文书写是没有分隔符的，所以分词的问题就比较突出。分词常用的手段可以是基于字典的最长串匹配，据说可以解决85%的问题，但是歧义分词很难。另外就是当下主流的统计机器学习的办法。</p><h3 id="词性标注（Label）"><a href="#词性标注（Label）" class="headerlink" title="词性标注（Label）"></a>词性标注（Label）</h3><p>基于机器学习的方法里，往往需要对词的词性进行标注。标注的目的是用来表示，词的一种隐状态，隐藏状态构成的转移就构成了状态转移序列。例如：苏宁易购/n 投资/v 了/u 国际米兰/n。其中，n代表名词，v代表动词，n,v都是标注。以此类推。</p><h3 id="命名实体识别（Named-Entity-Recognition）"><a href="#命名实体识别（Named-Entity-Recognition）" class="headerlink" title="命名实体识别（Named Entity Recognition）"></a>命名实体识别（Named Entity Recognition）</h3><p>本质上还是标注问题的一种。只不过把标注细化了。比如，苏宁/cmp_s 易购/cmp_e 是/v B2C/n 电商/n。我们把苏宁易购 标注成cmp_s和cmp_e,分别表征公司名的起始和结束。这样，当遇上苏宁/云商/易购这种场景时，也可以完整得识别出它是一个公司名称。如果，按照传统的标注方式，苏宁/cmp 易购/cmp这样笼统地标注可能会有问题。</p><h3 id="句法分析（Syntax-Parsing）"><a href="#句法分析（Syntax-Parsing）" class="headerlink" title="句法分析（Syntax Parsing）"></a>句法分析（Syntax Parsing）</h3><p>句法分析往往是一种基于规则的专家系统。当然也不是说它不能用统计学的方法进行构建，不过最初的时候，还是利用语言学专家的知识来构建的。句法分析的目的是解析句子的中各个成分的依赖关系。所以，往往最终生成的结果，是一棵句法分析树。句法分析可以解决传统词袋模型不考虑上下文的问题。比如，张三是李四的领导；李四是张三的领导。这两句话，用词袋模型是完全相同的，但是句法分析可以分析出其中的主从关系，真正理清句子的关系。</p><h3 id="指代消解-Anaphora-Resolution"><a href="#指代消解-Anaphora-Resolution" class="headerlink" title="指代消解(Anaphora Resolution)"></a>指代消解(Anaphora Resolution)</h3><p>中文中代词出现的频率很高，它的作用的是用来表征前文出现过的人名、地名等词。例如，苏宁易购坐落在南京，这家公司目前位于中国B2C市场前三。在这句话中，其实“苏宁易购”这个词出现了2次，“这家公司”指代的就是苏宁易购。但是出于中文的习惯，我们不会把“苏宁易购”再重复一遍。</p><h2 id="AI模型篇"><a href="#AI模型篇" class="headerlink" title="AI模型篇"></a>AI模型篇</h2><h3 id="Deep-Semantic-Similarity-Model-DSSM"><a href="#Deep-Semantic-Similarity-Model-DSSM" class="headerlink" title="Deep Semantic Similarity Model(DSSM)"></a>Deep Semantic Similarity Model(DSSM)</h3><h3 id="Triplet-loss"><a href="#Triplet-loss" class="headerlink" title="Triplet loss"></a>Triplet loss</h3><h3 id="Machine-Reading-Comprehension-MRC"><a href="#Machine-Reading-Comprehension-MRC" class="headerlink" title="Machine Reading Comprehension(MRC)"></a>Machine Reading Comprehension(MRC)</h3><h3 id="Knowledge-Base-QA-KBQA"><a href="#Knowledge-Base-QA-KBQA" class="headerlink" title="Knowledge Base-QA(KBQA)"></a>Knowledge Base-QA(KBQA)</h3><ul><li>WordNet(1998)</li><li>Freebase(2008)</li><li>Yago(2007)</li></ul><h3 id="Knowledge-base-completion-KBC"><a href="#Knowledge-base-completion-KBC" class="headerlink" title="Knowledge base completion(KBC)"></a>Knowledge base completion(KBC)</h3><h2 id="chatbot领域学习地图："><a href="#chatbot领域学习地图：" class="headerlink" title="chatbot领域学习地图："></a>chatbot领域学习地图：</h2><p><img src="http://ww1.sinaimg.cn/large/ca26ff18ly1fue50ug6o9j217m88pb2c.jpg" alt=""></p><blockquote><p>参考与引用</p><ol><li><a href="https://www.microsoft.com/en-us/research/publication/neural-approaches-to-conversational-ai/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/publication/neural-approaches-to-conversational-ai/</a></li><li><a href="https://chatbotsmagazine.com/chatbot-vocabulary-10-chatbot-terms-you-need-to-know-3911b1ef31b4" target="_blank" rel="noopener">https://chatbotsmagazine.com/chatbot-vocabulary-10-chatbot-terms-you-need-to-know-3911b1ef31b4</a></li><li><a href="https://blog.ubisend.com/discover-chatbots/chatbot-glossary" target="_blank" rel="noopener">https://blog.ubisend.com/discover-chatbots/chatbot-glossary</a></li><li><a href="https://blog.csdn.net/wangongxi/article/details/52662177" target="_blank" rel="noopener">https://blog.csdn.net/wangongxi/article/details/52662177</a></li><li><a href="https://www.jianshu.com/p/d7ec29abbcb8" target="_blank" rel="noopener">https://www.jianshu.com/p/d7ec29abbcb8</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=&quot;/awesome-chatbot/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;h2 id=&quot;chatbot基本篇&quot;&gt;&lt;a href=&quot;#chatbot基本篇&quot; class=&quot;headerlink&quot; title=&quot;chatbo
      
    
    </summary>
    
      <category term="AI" scheme="https://bupt.github.io/awesome-chatbot/categories/AI/"/>
    
    
      <category term="chatbot" scheme="https://bupt.github.io/awesome-chatbot/tags/chatbot/"/>
    
      <category term="conversationalAI" scheme="https://bupt.github.io/awesome-chatbot/tags/conversationalAI/"/>
    
      <category term="nlp" scheme="https://bupt.github.io/awesome-chatbot/tags/nlp/"/>
    
  </entry>
  
</feed>
